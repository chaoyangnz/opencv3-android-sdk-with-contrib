// This file is auto-generated. Do not edit!

#include "precomp.hpp"
#include "cvconfig.h"
#include "opencl_kernels_dnn.hpp"

#ifdef HAVE_OPENCL

namespace cv
{
namespace ocl
{
namespace dnn
{

static const char* const moduleName = "dnn";

struct cv::ocl::internal::ProgramEntry activations_oclsrc={moduleName, "activations",
"__kernel void ReLUForward(const int count, __global const T* in, __global T* out\n"
"#ifndef RELU_NO_SLOPE\n"
", T negative_slope\n"
"#endif\n"
") {\n"
"int index = get_global_id(0);\n"
"if(index < count)\n"
"#ifndef RELU_NO_SLOPE\n"
"out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n"
"#else\n"
"out[index] = in[index] > 0 ? in[index] : 0;\n"
"#endif\n"
"}\n"
"__kernel void TanHForward(const int count, __global T* in, __global T* out) {\n"
"int index = get_global_id(0);\n"
"if(index < count)\n"
"out[index] = tanh(in[index]);\n"
"}\n"
"__kernel void SigmoidForward(const int count, __global const T* in, __global T* out) {\n"
"int index = get_global_id(0);\n"
"if(index < count)\n"
"out[index] = 1.0f / (1.0f + exp(-in[index]));\n"
"}\n"
"__kernel void BNLLForward(const int n, __global const T* in, __global T* out) {\n"
"int index = get_global_id(0);\n"
"if (index < n) {\n"
"out[index] = in[index] > 0 ? in[index] + log(1.0f + exp(-in[index])) : log(1.0f + exp(in[index]));\n"
"}\n"
"}\n"
"__kernel void AbsValForward(const int n, __global const T* in, __global T* out) {\n"
"int index = get_global_id(0);\n"
"if (index < n)\n"
"out[index] = fabs(in[index]);\n"
"}\n"
"__kernel void PowForward(const int n, __global const T* in, __global T* out, const T power, const T scale, const T shift) {\n"
"int index = get_global_id(0);\n"
"if (index < n)\n"
"out[index] = pow(shift + scale * in[index], power);\n"
"}\n"
, "931ebfde0168e305242fd6d4b85ba396", NULL};
struct cv::ocl::internal::ProgramEntry batchnorm_oclsrc={moduleName, "batchnorm",
"__kernel void batchnorm(__global const T *src, int src_offset,\n"
"__global const float *meanMat,\n"
"float varMeanScale,\n"
"__global const float *invStdMat,\n"
"__global const float *weight,\n"
"__global const float *bias,\n"
"int hasWeight, int hasBias,\n"
"int width, int height, int channel,\n"
"__global T *dst, int dst_offset)\n"
"{\n"
"int x = get_global_id(0);\n"
"int y = get_global_id(1);\n"
"int c = get_global_id(2);\n"
"if (x >= width || y >= height || c >= channel)\n"
"return;\n"
"float mean = meanMat[c] * varMeanScale;\n"
"float invstd = invStdMat[c];\n"
"float w = hasWeight ? weight[c] : 1;\n"
"float b = hasBias ? bias[c] : 0;\n"
"int index = y * width + x + c * width * height;\n"
"T val = (src[index + src_offset] - mean) * w * invstd + b;\n"
"dst[index + dst_offset] = val;\n"
"}\n"
, "e3277501967e5fc31094dc5a15c85612", NULL};
struct cv::ocl::internal::ProgramEntry col2im_oclsrc={moduleName, "col2im",
"/*************************************************************************************\n"
"* Copyright (c) 2015, Advanced Micro Devices, Inc.\n"
"* All rights reserved.\n"
"*\n"
"* Redistribution and use in source and binary forms, with or without modification,\n"
"* are permitted provided that the following conditions are met:\n"
"*\n"
"* 1. Redistributions of source code must retain the above copyright notice, this\n"
"* list of conditions and the following disclaimer.\n"
"*\n"
"* 2. Redistributions in binary form must reproduce the above copyright notice,\n"
"* this list of conditions and the following disclaimer in the documentation and/or\n"
"*  other materials provided with the distribution.\n"
"*\n"
"* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n"
"* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n"
"* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n"
"* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n"
"* INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n"
"* BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n"
"* OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n"
"* WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n"
"* ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n"
"* POSSIBILITY OF SUCH DAMAGE.\n"
"**************************************************************************************/\n"
"__kernel void col2im(const int n, __global const T* data_col, const int col_offset,\n"
"const int height, const int width, const int channels,\n"
"const int patch_h, const int patch_w,\n"
"const int pad_h, const int pad_w,\n"
"const int stride_h, const int stride_w,\n"
"const int height_col, const int width_col,\n"
"__global T* data_im, const int img_offset)\n"
"{\n"
"data_col = data_col + col_offset;\n"
"data_im = data_im + img_offset;\n"
"int index = get_global_id(0);\n"
"if(index < n) {\n"
"T val = 0;\n"
"int w = index % width + pad_w;\n"
"int h = (index / width) % height + pad_h;\n"
"int c = index / (width * height);\n"
"int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;\n"
"int w_col_end = min(w / stride_w + 1, width_col);\n"
"int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;\n"
"int h_col_end = min(h / stride_h + 1, height_col);\n"
"int offset =\n"
"(c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;\n"
"int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;\n"
"int coeff_w_col = (1 - stride_w * height_col * width_col);\n"
"for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {\n"
"for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {\n"
"val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];\n"
"}\n"
"}\n"
"data_im[index] = val;\n"
"}\n"
"}\n"
, "9728075ae8cdfd546b072425e0d804a6", NULL};
struct cv::ocl::internal::ProgramEntry concat_oclsrc={moduleName, "concat",
"__kernel void concat(const int nthreads,\n"
"__global const Dtype* in_data,\n"
"const int num_concats,\n"
"const int concat_size,\n"
"const int top_concat_axis,\n"
"const int bottom_concat_axis,\n"
"const int offset_concat_axis,\n"
"__global Dtype* out_data) {\n"
"for (int index = get_global_id(0); index < nthreads;\n"
"index += get_global_size(0)) {\n"
"const int total_concat_size = concat_size * bottom_concat_axis;\n"
"const int concat_num = index / total_concat_size;\n"
"const int concat_index = index % total_concat_size;\n"
"const int top_index = concat_index\n"
"+ (concat_num * top_concat_axis + offset_concat_axis) * concat_size;\n"
"out_data[top_index] = in_data[index];\n"
"}\n"
"}\n"
, "dbeb9325b2a131a400d6eb25516d8443", NULL};
struct cv::ocl::internal::ProgramEntry conv_layer_spatial_oclsrc={moduleName, "conv_layer_spatial",
"#if APPLY_BIAS\n"
"#define BIAS_KERNEL_ARG __global Dtype * biases_base,\n"
"#else\n"
"#define BIAS_KERNEL_ARG\n"
"#endif\n"
"#if defined(FUSED_CONV_RELU)\n"
"#define ACTIVATION_RELU_FUNCTION(x, c) ((Dtype)(x) > 0 ? (Dtype)(x) : ((Dtype)(x) * (Dtype)(negative_slope)))\n"
"#define NEGATIVE_SLOPE_ARG Dtype negative_slope,\n"
"#elif defined(FUSED_CONV_PRELU)\n"
"#define ACTIVATION_RELU_FUNCTION(x, c) ((Dtype)(x) > 0 ? (Dtype)(x) : ((Dtype)(x) * (Dtype)(negative_slope[c])))\n"
"#define NEGATIVE_SLOPE_ARG __global const Dtype *negative_slope,\n"
"#else\n"
"#define ACTIVATION_RELU_FUNCTION(x, c) (x)\n"
"#define NEGATIVE_SLOPE_ARG\n"
"#endif\n"
"#define ACTIVATION_FUNCTION(_dst_, _offset_, _data_, _channel_) do { (_dst_)[(_offset_)] = ACTIVATION_RELU_FUNCTION(_data_, _channel_);} while(0)\n"
"#define __CAT(x, y) x##y\n"
"#define CAT(x, y) __CAT(x, y)\n"
"#define LOOP0(VAR, STMT)\n"
"#define LOOP1(VAR, STMT) (STMT); (VAR)++;\n"
"#define LOOP2(VAR, STMT) LOOP1(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP3(VAR, STMT) LOOP2(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP4(VAR, STMT) LOOP3(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP5(VAR, STMT) LOOP4(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP6(VAR, STMT) LOOP5(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP7(VAR, STMT) LOOP6(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP8(VAR, STMT) LOOP7(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP9(VAR, STMT) LOOP8(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP10(VAR, STMT) LOOP9(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP11(VAR, STMT) LOOP10(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP12(VAR, STMT) LOOP11(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP13(VAR, STMT) LOOP12(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP14(VAR, STMT) LOOP13(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP15(VAR, STMT) LOOP14(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP16(VAR, STMT) LOOP15(VAR, STMT); (STMT); (VAR)++;\n"
"#define LOOP(N, VAR, STMT) CAT(LOOP, N)((VAR), (STMT))\n"
"#if defined(convolve_simd) || defined(Conv_Interleaved)\n"
"#if Dtype_SIZE == 4\n"
"#define INT_TYPE uint\n"
"#define INT_TYPE2 uint2\n"
"#define INT_TYPE4 uint4\n"
"#define INT_TYPE8 uint8\n"
"#define SUB_GROUP_BLOCK_READ2 intel_sub_group_block_read2\n"
"#define SUB_GROUP_BLOCK_READ4 intel_sub_group_block_read4\n"
"#define SUB_GROUP_BLOCK_READ8 intel_sub_group_block_read8\n"
"#define SUB_GROUP_BLOCK_READ intel_sub_group_block_read\n"
"#else\n"
"#error \"Unsupported type\"\n"
"#endif\n"
"#endif\n"
"#ifdef KERNEL_BASIC\n"
"__kernel void ConvolveBasic(\n"
"NEGATIVE_SLOPE_ARG\n"
"__global Dtype* image_data,\n"
"int image_offset,\n"
"__global Dtype* kernel_data,\n"
"int kernel_offset,\n"
"__global Dtype* bias,\n"
"const int bias_offset,\n"
"__global Dtype* convolved_image,\n"
"const int convolved_image_offset,\n"
"const ushort input_width,\n"
"const ushort input_height,\n"
"const ushort output_width,\n"
"const ushort output_height,\n"
"const ushort pad_w,\n"
"const ushort pad_h\n"
")\n"
"{\n"
"const int outputX = get_global_id(0);\n"
"const int outputY = get_global_id(1);\n"
"const int kernelNum = get_global_id(2) * ZPAR;\n"
"if (outputX < output_width && outputY < output_height)\n"
"{\n"
"Dtype sum[ZPAR];\n"
"for (int kern = 0; kern < ZPAR; kern++)\n"
"{\n"
"sum[kern] = 0.0f;\n"
"}\n"
"const int org_y = outputY * STRIDE_Y - pad_h;\n"
"const int org_x = outputX * STRIDE_X - pad_w;\n"
"const int currentKernelOffset = kernel_offset + kernelNum*KERNEL_HEIGHT*KERNEL_WIDTH*CHANNELS;\n"
"#if APPLY_BIAS\n"
"const int biasIndex = bias_offset + kernelNum;\n"
"#endif\n"
"const int local_image_offset = org_y * input_width + org_x;\n"
"const int imageSize = input_width * input_height;\n"
"__global Dtype* image_dataPtr = (image_data + (image_offset + local_image_offset));\n"
"__global Dtype* kernel_dataPtr = (kernel_data + (currentKernelOffset));\n"
"for (int c = 0; c < CHANNELS; c++)\n"
"{\n"
"for (int y = 0; y < KERNEL_HEIGHT; y++)\n"
"{\n"
"for (int x = 0; x < KERNEL_WIDTH; x++)\n"
"{\n"
"int y_ = org_y + y * DILATION_Y;\n"
"int x_ = org_x + x * DILATION_X;\n"
"if (!(y_ >= 0 && y_ < input_height && x_ >= 0 && x_ < input_width))\n"
"{\n"
"continue;\n"
"}\n"
"for (int kern = 0; kern < ZPAR; kern++)\n"
"{\n"
"sum[kern] += image_dataPtr[x * DILATION_X] * kernel_dataPtr[kern*KERNEL_HEIGHT*KERNEL_WIDTH*CHANNELS + x];\n"
"}\n"
"}\n"
"image_dataPtr += input_width * DILATION_Y;\n"
"kernel_dataPtr += KERNEL_WIDTH;\n"
"}\n"
"image_dataPtr += imageSize - input_width*KERNEL_HEIGHT*DILATION_Y;\n"
"}\n"
"for (int kern = 0; kern < ZPAR; kern++)\n"
"{\n"
"if (kernelNum + kern < OUTPUT_Z)\n"
"{\n"
"int offset = convolved_image_offset + (kernelNum+kern)*output_height*output_width + outputY*output_width + outputX;\n"
"#if APPLY_BIAS\n"
"ACTIVATION_FUNCTION(convolved_image, offset, sum[kern] + bias[biasIndex + kern], biasIndex + kern);\n"
"#else\n"
"ACTIVATION_FUNCTION(convolved_image, offset, sum[kern], biasIndex + kern);\n"
"#endif\n"
"}\n"
"}\n"
"}\n"
"}\n"
"#elif defined KERNEL_IDLF\n"
"#if TYPE == TYPE_HALF\n"
"#define VLOAD4(_v, _p) do { (_v).s0 = *(_p); (_v).s1 = *(_p + 1); (_v).s2 = *(_p + 2); (_v).s3 = *(_p + 3); } while(0)\n"
"#else\n"
"#define VLOAD4(_v, _p) do { _v = vload4(0, _p); } while(0)\n"
"#endif\n"
"#ifndef __BEIGNET__\n"
"__attribute__((reqd_work_group_size(1, 1, SIMD_SIZE)))\n"
"__attribute__((intel_reqd_sub_group_size(SIMD_SIZE)))\n"
"#endif\n"
"__kernel void\n"
"convolve_simd(\n"
"NEGATIVE_SLOPE_ARG\n"
"__global Dtype* inputs_base,\n"
"filter_qualifier Dtype* weights_base,\n"
"BIAS_KERNEL_ARG\n"
"__global Dtype* outputs_base,\n"
"const ushort input_width,\n"
"const ushort input_height,\n"
"const ushort output_width,\n"
"const ushort output_height)\n"
"{\n"
"__global Dtype* outputs = outputs_base;\n"
"__global Dtype* inputs = inputs_base;\n"
"filter_qualifier Dtype* weights = weights_base;\n"
"unsigned int oc = get_global_id(0) * OUT_BLOCK_WIDTH;\n"
"unsigned int or = get_global_id(1) * OUT_BLOCK_HEIGHT;\n"
"unsigned int fm = get_global_id(2);\n"
"unsigned int fmg = get_group_id(2);\n"
"unsigned int lid = get_local_id(2);\n"
"Dtype out[OUT_BLOCK_WIDTH * OUT_BLOCK_HEIGHT];\n"
"int in_addr;\n"
"unsigned int weight_addr = (fmg % (ALIGNED_NUM_FILTERS/SIMD_SIZE)) * INPUT_DEPTH * KERNEL_WIDTH * KERNEL_HEIGHT * SIMD_SIZE + lid;\n"
"for(int i=0;i<OUT_BLOCK_SIZE;i++) {\n"
"out[i]=0.0f;\n"
"}\n"
"unsigned int num_in_batch = ( fm ) / ALIGNED_NUM_FILTERS;\n"
"unsigned int input_batch_offset = num_in_batch * input_height * input_width * TOTAL_INPUT_DEPTH_SIZE;\n"
"int curr_local_y = ( lid / ( TILE_X / 4 ) );\n"
"int curr_local_x = ( lid % ( TILE_X / 4 ) ) * 4;\n"
"int curr_y = or * STRIDE_Y + INPUT_START_Y + curr_local_y;\n"
"int curr_x = oc * STRIDE_X + INPUT_START_X + curr_local_x;\n"
"#if INPUT_PAD_W != 0 || INPUT_PAD_H != 0\n"
"int saved_y = curr_y;\n"
"#endif\n"
"in_addr = input_batch_offset + INPUT_START_Z * input_height * input_width\n"
"+  (curr_y - INPUT_PAD_H) * input_width\n"
"+   curr_x - INPUT_PAD_W;\n"
"union {\n"
"Dtype4 in_vec[INVEC_SIZE];\n"
"Dtype in_array[INVEC_SIZE * 4];\n"
"} in_buf;\n"
"for(int kd = 0; kd < INPUT_DEPTH; kd++)\n"
"{\n"
"int in_offset = in_addr;\n"
"int reg = 0;\n"
"LOOP(INVEC_SIZE, reg,\n"
"{\n"
"if (curr_local_y + reg * TILE_Y_STRIDE < TILE_Y || INVEC_SIZE * TILE_Y_STRIDE <= (TILE_Y + 2) || reg < INVEC_SIZE - 1) {\n"
"#if INPUT_PAD_W != 0 || INPUT_PAD_H != 0\n"
"if (curr_y >= INPUT_PAD_H && curr_y < input_height + INPUT_PAD_H && curr_x + 3 >= INPUT_PAD_W && curr_x < input_width + INPUT_PAD_W) {\n"
"if (curr_x < INPUT_PAD_W) {\n"
"in_buf.in_vec[reg].s0 = 0;\n"
"if (curr_x + 1 >= INPUT_PAD_W)\n"
"in_buf.in_vec[reg].s1 = *(inputs + in_offset + 1);\n"
"else\n"
"in_buf.in_vec[reg].s1 = 0;\n"
"if (curr_x + 2 >= INPUT_PAD_W)\n"
"in_buf.in_vec[reg].s2 = *(inputs + in_offset + 2);\n"
"else\n"
"in_buf.in_vec[reg].s2 = 0;\n"
"in_buf.in_vec[reg].s3 = *(inputs + in_offset + 3);\n"
"} else {\n"
"VLOAD4(in_buf.in_vec[reg], inputs + in_offset);\n"
"if (curr_x + 1 >= input_width + INPUT_PAD_W)\n"
"in_buf.in_vec[reg].s1 = 0;\n"
"if (curr_x + 2 >= input_width + INPUT_PAD_W)\n"
"in_buf.in_vec[reg].s2 = 0;\n"
"if (curr_x + 3 >= input_width + INPUT_PAD_W)\n"
"in_buf.in_vec[reg].s3 = 0;\n"
"}\n"
"} else {\n"
"in_buf.in_vec[reg] = 0;\n"
"}\n"
"curr_y += TILE_Y_STRIDE;\n"
"#else\n"
"VLOAD4(in_buf.in_vec[reg], inputs + in_offset);\n"
"#endif\n"
"}\n"
"in_offset += input_width * TILE_Y_STRIDE;\n"
"});\n"
"in_addr += input_height * input_width;\n"
"#if INPUT_PAD_W != 0 || INPUT_PAD_H != 0\n"
"curr_y = saved_y;\n"
"#endif\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT != 1\n"
"#define WEIGHT_PREF 8\n"
"#else\n"
"#define WEIGHT_PREF 1\n"
"#endif\n"
"union {\n"
"Dtype w[WEIGHT_PREF];\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT != 1\n"
"INT_TYPE8 ui8;\n"
"#endif\n"
"} weight_buf;\n"
"int w_idx=0;\n"
"unsigned int orig_weight_addr = weight_addr;\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT != 1\n"
"weight_buf.ui8 = SUB_GROUP_BLOCK_READ8((__global INT_TYPE *)&weights[weight_addr]);\n"
"weight_addr += SIMD_SIZE * WEIGHT_PREF;\n"
"#else\n"
"weight_buf.w[0] = as_Dtype(SUB_GROUP_BLOCK_READ((__global INT_TYPE *)&weights[weight_addr]));\n"
"weight_addr += SIMD_SIZE * 1;\n"
"#endif\n"
"#define BLOCK_IN(n) sub_group_broadcast( in_buf.in_array[((n)%4) + ((n) / (TILE_Y_STRIDE * TILE_X)) * 4], (((n) % (TILE_Y_STRIDE * TILE_X))/4))\n"
"int kr = 0;\n"
"LOOP(KERNEL_HEIGHT, kr,\n"
"{\n"
"int kc = 0;\n"
"LOOP(KERNEL_WIDTH, kc,\n"
"{\n"
"for(int br=0; br < OUT_BLOCK_HEIGHT; br++) {\n"
"for(int bc=0; bc < OUT_BLOCK_WIDTH; bc++) {\n"
"Dtype input = BLOCK_IN((br * STRIDE_Y + kr * DILATION_Y) * TILE_X + bc * STRIDE_X + kc * DILATION_X);\n"
"out[br * OUT_BLOCK_WIDTH + bc] = mad(weight_buf.w[w_idx % WEIGHT_PREF], input, out[br * OUT_BLOCK_WIDTH + bc]);\n"
"}\n"
"}\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT > WEIGHT_PREF\n"
"if ((w_idx + 1) % WEIGHT_PREF == 0\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT % 8 != 0\n"
"&& ((w_idx + 1) <= (KERNEL_WIDTH * KERNEL_HEIGHT - WEIGHT_PREF))\n"
"#endif\n"
") {\n"
"weight_buf.ui8 = SUB_GROUP_BLOCK_READ8((__global INT_TYPE *)&weights[weight_addr]);\n"
"weight_addr += SIMD_SIZE * WEIGHT_PREF;\n"
"}\n"
"#if KERNEL_WIDTH*KERNEL_HEIGHT % 8 == 0\n"
"#else\n"
"else if ((w_idx + 1) %  WEIGHT_PREF == 0 && ((w_idx + 1) > (KERNEL_WIDTH * KERNEL_HEIGHT - WEIGHT_PREF)))\n"
"#if KERNEL_WIDTH * KERNEL_HEIGHT % 8 == 1\n"
"weight_buf.w[0] = weights[weight_addr];\n"
"#elif KERNEL_WIDTH * KERNEL_HEIGHT % 8 == 2\n"
"weight_buf.ui8.s01 = SUB_GROUP_BLOCK_READ2((__global INT_TYPE *)&weights[weight_addr]);\n"
"#elif KERNEL_WIDTH * KERNEL_HEIGHT % 8 <= 4\n"
"weight_buf.ui8.s0123 = SUB_GROUP_BLOCK_READ4((__global INT_TYPE *)&weights[weight_addr]);\n"
"#else\n"
"weight_buf.ui8 = SUB_GROUP_BLOCK_READ8((__global INT_TYPE *)&weights[weight_addr]);\n"
"#endif\n"
"#endif\n"
"#endif\n"
"++w_idx;\n"
"});\n"
"});\n"
"weight_addr = orig_weight_addr + KERNEL_WIDTH * KERNEL_HEIGHT * SIMD_SIZE;\n"
"}\n"
"if (ALIGNED_NUM_FILTERS != NUM_FILTERS && fm > 0xfffffffeul) {\n"
"outputs[0] = BLOCK_IN(fm % SIMD_SIZE);\n"
"}\n"
"fm = fm % ALIGNED_NUM_FILTERS;\n"
"if ((ALIGNED_NUM_FILTERS == NUM_FILTERS || fm < NUM_FILTERS)) {\n"
"unsigned int out_addr = OUT_BUFF_OFFSET + ( num_in_batch * TOTAL_OUTPUT_DEPTH + fm ) * output_width * output_height;\n"
"out_addr += or * output_width + oc;\n"
"#if APPLY_BIAS\n"
"Dtype bias = biases_base[fm];\n"
"#else\n"
"Dtype bias = 0;\n"
"#endif\n"
"for(unsigned int r = 0; r < OUT_BLOCK_HEIGHT; r++) {\n"
"if (r + or >= output_height) break;\n"
"for(unsigned int c = 0; c < OUT_BLOCK_WIDTH; c++) {\n"
"if (c + oc >= output_width) break;\n"
"ACTIVATION_FUNCTION(outputs, out_addr + r * output_width + c, bias + out[r * OUT_BLOCK_WIDTH + c], fm);\n"
"}\n"
"}\n"
"}\n"
"}\n"
"#else\n"
"#if APPLY_BIAS\n"
"#define SUBGROUP_GET_BIAS(k, i) intel_sub_group_shuffle(bias[k], i)\n"
"#else\n"
"#define SUBGROUP_GET_BIAS(k, i) ((Dtype)0)\n"
"#endif\n"
"#ifdef Conv_Interleaved\n"
"typedef struct float1 { float s0; } float1;\n"
"typedef struct float5 { float s0; float s1; float s2; float s3; float s4; } float5;\n"
"typedef struct float6 { float s0; float s1; float s2; float s3; float s4; float s5; } float6;\n"
"typedef struct float7 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; } float7;\n"
"typedef struct float9 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; float s7; float s8; } float9;\n"
"typedef struct float10 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9;} float10;\n"
"typedef struct float11 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9; float sa;} float11;\n"
"typedef struct float12 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9; float sa; float sb; } float12;\n"
"typedef struct float13 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9; float sa; float sb; float sc;} float13;\n"
"typedef struct float14 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; } float14;\n"
"typedef struct float15 { float s0; float s1; float s2; float s3; float s4; float s5;\n"
"float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; float se; } float15;\n"
"typedef struct float0 { float s0; } float0;\n"
"#define OUT_PITCH_X output_width\n"
"#define ROW_PITCH input_width\n"
"#define GEMM_LIKE_KERNEL_ARGS     \\\n"
"NEGATIVE_SLOPE_ARG            \\\n"
"const __global Dtype *src0,   \\\n"
"const __global Dtype *src1,   \\\n"
"BIAS_KERNEL_ARG               \\\n"
"__global Dtype *dst,          \\\n"
"const ushort input_width,     \\\n"
"const ushort input_height,    \\\n"
"const ushort output_width,    \\\n"
"const ushort output_height,   \\\n"
"const int out_pitch_y,     \\\n"
"const int out_pitch_z,     \\\n"
"const int aligned_input_size, \\\n"
"const int slice_pitch\n"
"#endif\n"
"#ifdef GEMM_LIKE_CONV_32_1\n"
"#define TILE_M          1\n"
"#define TILE_K          KERNEL_WIDTH\n"
"#define TILE_N          32\n"
"#ifndef __BEIGNET__\n"
"__attribute__((intel_reqd_sub_group_size(8)))\n"
"#endif\n"
"__kernel void Conv_Interleaved(GEMM_LIKE_KERNEL_ARGS)\n"
"{\n"
"const int group_x = get_group_id(0);\n"
"const int group_y = get_group_id(1);\n"
"const int global_x = get_global_id(0);\n"
"const int global_y = get_global_id(1);\n"
"const int global_z = get_global_id(2);\n"
"int interleaved_y;\n"
"int kernel_y;\n"
"int kernel_idx;\n"
"#define DOT_PRODUCT_8( _result, _rowA, colB )    \\\n"
"{   \\\n"
"_result.s0 = mad( _rowA, sub_group_broadcast( colB, 0 ), _result.s0 );  \\\n"
"_result.s1 = mad( _rowA, sub_group_broadcast( colB, 1 ), _result.s1 );  \\\n"
"_result.s2 = mad( _rowA, sub_group_broadcast( colB, 2 ), _result.s2 );  \\\n"
"_result.s3 = mad( _rowA, sub_group_broadcast( colB, 3 ), _result.s3 );  \\\n"
"_result.s4 = mad( _rowA, sub_group_broadcast( colB, 4 ), _result.s4 );  \\\n"
"_result.s5 = mad( _rowA, sub_group_broadcast( colB, 5 ), _result.s5 );  \\\n"
"_result.s6 = mad( _rowA, sub_group_broadcast( colB, 6 ), _result.s6 );  \\\n"
"_result.s7 = mad( _rowA, sub_group_broadcast( colB, 7 ), _result.s7 );  \\\n"
"}\n"
"typedef CAT( Dtype, KERNEL_WIDTH ) Dtype_t;\n"
"if( TILE_N_LAST == 0 || global_x < WIDTH1 / TILE_N )\n"
"{\n"
"Dtype8  blockC00 = 0.f;\n"
"Dtype8  blockC10 = 0.f;\n"
"Dtype8  blockC20 = 0.f;\n"
"Dtype8  blockC30 = 0.f;\n"
"int curr_x = ( global_y % output_width ) * STRIDE_X;\n"
"int curr_y = ( global_y / output_width ) * STRIDE_Y;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"int saved_y = curr_y;\n"
"#endif\n"
"const __global Dtype *src0_read = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y - INPUT_PAD_H) * ROW_PITCH\n"
"+ (curr_x - INPUT_PAD_W);\n"
"const __global Dtype *src1_read = src1 + ( global_x * TILE_N  * 2);\n"
"int patch_depth = 0;\n"
"do\n"
"{\n"
"int patch_row = 0;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"curr_y = saved_y;\n"
"#endif\n"
"do\n"
"{\n"
"const bool kernel_width_is_odd = KERNEL_WIDTH % 2 == 1;\n"
"#if INPUT_PAD_W == 0 && INPUT_PAD_H == 0 && DILATION_X == 1 && DILATION_Y == 1\n"
"Dtype_t blockA00 = ( (const __global Dtype_t*)src0_read )[  0  ];\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"#else\n"
"Dtype_t blockA00;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"int pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y >= INPUT_PAD_H && curr_y < input_height + INPUT_PAD_H && curr_x + pos * DILATION_X >= INPUT_PAD_W && curr_x + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA00[pos] = src0_read[pos * DILATION_X];\n"
"else\n"
"pblockA00[pos] = 0;\n"
"})\n"
"curr_y += DILATION_Y;\n"
"#endif\n"
"src0_read += (ROW_PITCH * DILATION_Y);\n"
"Dtype blockB00[KERNEL_WIDTH*4];\n"
"Dtype8* p8BlockB00 = (Dtype8*)blockB00;\n"
"Dtype4* p4BlockB00 = (Dtype4*)blockB00;\n"
"Dtype*  pBlockB00 =  (Dtype* )blockB00;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"p8BlockB00[interleaved_y] = as_Dtype8( SUB_GROUP_BLOCK_READ8( (const __global INT_TYPE *)src1_read ) );\n"
"src1_read += WIDTH1 * 2;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"p4BlockB00[KERNEL_WIDTH - 1] = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE *)src1_read ) );\n"
"src1_read += WIDTH1 * 2;\n"
"}\n"
"kernel_idx = 0;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"} )\n"
"kernel_y = interleaved_y * 2;\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"}\n"
"}\n"
"while( ++patch_row < KERNEL_HEIGHT );\n"
"src0_read += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y);\n"
"}\n"
"while ( ++patch_depth < INPUT_DEPTH );\n"
"int out_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M ) / output_width + OUT_PADDING_HEIGHT) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M ) % output_width ) + OUT_PADDING_LEFT;\n"
"__global Dtype *out = dst + out_offset;\n"
"#if APPLY_BIAS\n"
"Dtype bias[4];\n"
"Dtype4 *bias_vec;\n"
"bias_vec = (Dtype4*)bias;\n"
"*bias_vec = as_Dtype4(SUB_GROUP_BLOCK_READ4((__global INT_TYPE *)biases_base + group_x * TILE_N));\n"
"#endif\n"
"if (global_y * TILE_M < output_width * output_height )\n"
"{\n"
"for (int i = 0; i < 8; i++)\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 0 + i ) * out_pitch_y, blockC00[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 8 + i ) * out_pitch_y, blockC10[i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + 8 + i);\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 16 + i ) * out_pitch_y, blockC20[i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + 16 + i);\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 24 + i ) * out_pitch_y, blockC30[i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + 24 + i);\n"
"}\n"
"}\n"
"}\n"
"#if TILE_N_LAST > 0\n"
"else\n"
"{\n"
"int i = 0;\n"
"Dtype8  blockC[TILE_N_LAST_DIV8];\n"
"LOOP(TILE_N_LAST_DIV8, i,\n"
"{\n"
"blockC[i] = 0.f;\n"
"} )\n"
"int curr_x = ( global_y % output_width ) * STRIDE_X;\n"
"int curr_y = ( global_y / output_width ) * STRIDE_Y;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"int saved_y = curr_y;\n"
"#endif\n"
"const __global Dtype *src0_read = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y - INPUT_PAD_H) * ROW_PITCH\n"
"+ (curr_x - INPUT_PAD_W);\n"
"const __global Dtype *src1_read = src1 + ( global_x * TILE_N  * 2);\n"
"int patch_depth = 0;\n"
"do\n"
"{\n"
"int patch_row = 0;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"curr_y = saved_y;\n"
"#endif\n"
"do\n"
"{\n"
"const bool kernel_width_is_odd = KERNEL_WIDTH % 2 == 1;\n"
"#if INPUT_PAD_W == 0 && INPUT_PAD_H == 0 && DILATION_X == 1 && DILATION_Y == 1\n"
"Dtype_t blockA00 = ( (const __global Dtype_t*)src0_read )[  0  ];\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"#else\n"
"Dtype_t blockA00;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"int pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y >= INPUT_PAD_H && curr_y < input_height + INPUT_PAD_H && curr_x + pos * DILATION_X >= INPUT_PAD_W && curr_x + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA00[pos] = src0_read[pos * DILATION_X];\n"
"else\n"
"pblockA00[pos] = 0;\n"
"})\n"
"curr_y += DILATION_Y;\n"
"#endif\n"
"src0_read += (ROW_PITCH * DILATION_Y);\n"
"Dtype blockB[KERNEL_WIDTH * TILE_N_LAST_DIV8];\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"#if TILE_N_LAST_DIV8 == 1\n"
"Dtype2* p2BlockB = (Dtype2* )blockB;\n"
"p2BlockB[interleaved_y] = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 2\n"
"Dtype4* p4BlockB = (Dtype4* )blockB;\n"
"p4BlockB[interleaved_y] = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 3\n"
"Dtype6* p6BlockB = (Dtype6* )blockB;\n"
"(*((Dtype8*)(&p6BlockB[interleaved_y]))).s0123 = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read ) );\n"
"(*((Dtype8*)(&p6BlockB[interleaved_y]))).s45 = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)(src1_read + 4 * 8) ) );\n"
"#endif\n"
"src1_read += WIDTH1 * 2;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"#if TILE_N_LAST_DIV8 == 1\n"
"Dtype* pBlockB = (Dtype* )blockB;\n"
"pBlockB[KERNEL_WIDTH - 1] = as_Dtype( SUB_GROUP_BLOCK_READ( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 2\n"
"Dtype2* p2BlockB = (Dtype2* )blockB;\n"
"p2BlockB[KERNEL_WIDTH - 1] = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 3\n"
"Dtype3* p3BlockB = (Dtype3* )blockB;\n"
"p3BlockB[KERNEL_WIDTH - 1].s01 = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"p3BlockB[KERNEL_WIDTH - 1].s2 = as_Dtype( SUB_GROUP_BLOCK_READ( (const __global INT_TYPE*) (src1_read + 2 * 8) ) );\n"
"#endif\n"
"src1_read += WIDTH1 * 2;\n"
"}\n"
"Dtype* pBlockB = (Dtype*)blockB;\n"
"kernel_idx = 0;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_8( blockC[0], pblockA00[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC[0], pblockA00[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 2\n"
"DOT_PRODUCT_8( blockC[1], pblockA00[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC[1], pblockA00[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 3\n"
"DOT_PRODUCT_8( blockC[2], pblockA00[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC[2], pblockA00[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#endif\n"
"#endif\n"
"} )\n"
"kernel_y = interleaved_y * 2;\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"DOT_PRODUCT_8( blockC[0], pblockA00[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 2\n"
"DOT_PRODUCT_8( blockC[1], pblockA00[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 3\n"
"DOT_PRODUCT_8( blockC[2], pblockA00[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#endif\n"
"#endif\n"
"}\n"
"}\n"
"while( ++patch_row < KERNEL_HEIGHT );\n"
"src0_read += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"}\n"
"while ( ++patch_depth < INPUT_DEPTH );\n"
"int out_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M ) / output_width + OUT_PADDING_HEIGHT) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M ) % output_width ) + OUT_PADDING_LEFT;\n"
"__global Dtype *out = dst + out_offset;\n"
"#if APPLY_BIAS\n"
"Dtype bias[4];\n"
"Dtype4 *bias_vec;\n"
"bias_vec = (Dtype4*)bias;\n"
"*bias_vec = as_Dtype4(SUB_GROUP_BLOCK_READ4((__global INT_TYPE *)biases_base + group_x * TILE_N));\n"
"#endif\n"
"if (global_y * TILE_M < output_width * output_height )\n"
"{\n"
"for (int i = 0; i < 8; i++)\n"
"{\n"
"if ( TILE_N_LAST_DIV8 > 0 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 0+i) * out_pitch_y, blockC[0][i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 1 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out_offset + ( 8+i) * out_pitch_y, blockC[1][i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 8);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 2 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out_offset + (16+i) * out_pitch_y, blockC[2][i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + i + 16);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 3 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out_offset + (24+i) * out_pitch_y, blockC[3][i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + i + 24);\n"
"}\n"
"}\n"
"}\n"
"}\n"
"#endif\n"
"}\n"
"#endif\n"
"#ifdef GEMM_LIKE_CONV_32_2\n"
"#define TILE_M          2\n"
"#define TILE_K          KERNEL_WIDTH\n"
"#define TILE_N          32\n"
"#ifndef __BEIGNET__\n"
"__attribute__((intel_reqd_sub_group_size(8)))\n"
"#endif\n"
"__kernel void Conv_Interleaved(GEMM_LIKE_KERNEL_ARGS)\n"
"{\n"
"const int group_x = get_group_id(0);\n"
"const int group_y = get_group_id(1);\n"
"const int global_x = get_global_id(0);\n"
"const int global_y = get_global_id(1);\n"
"const int global_z = get_global_id(2);\n"
"int interleaved_y;\n"
"int kernel_y;\n"
"int kernel_idx;\n"
"#define DOT_PRODUCT_8( _result, _rowA, colB )    \\\n"
"{   \\\n"
"_result.s0 = mad( _rowA, sub_group_broadcast( colB, 0 ), _result.s0 );  \\\n"
"_result.s1 = mad( _rowA, sub_group_broadcast( colB, 1 ), _result.s1 );  \\\n"
"_result.s2 = mad( _rowA, sub_group_broadcast( colB, 2 ), _result.s2 );  \\\n"
"_result.s3 = mad( _rowA, sub_group_broadcast( colB, 3 ), _result.s3 );  \\\n"
"_result.s4 = mad( _rowA, sub_group_broadcast( colB, 4 ), _result.s4 );  \\\n"
"_result.s5 = mad( _rowA, sub_group_broadcast( colB, 5 ), _result.s5 );  \\\n"
"_result.s6 = mad( _rowA, sub_group_broadcast( colB, 6 ), _result.s6 );  \\\n"
"_result.s7 = mad( _rowA, sub_group_broadcast( colB, 7 ), _result.s7 );  \\\n"
"}\n"
"typedef CAT( Dtype, KERNEL_WIDTH ) Dtype_t;\n"
"if( TILE_N_LAST == 0 || global_x < WIDTH1 / TILE_N )\n"
"{\n"
"Dtype8  blockC00 = 0.f;\n"
"Dtype8  blockC10 = 0.f;\n"
"Dtype8  blockC20 = 0.f;\n"
"Dtype8  blockC30 = 0.f;\n"
"Dtype8  blockC01 = 0.f;\n"
"Dtype8  blockC11 = 0.f;\n"
"Dtype8  blockC21 = 0.f;\n"
"Dtype8  blockC31 = 0.f;\n"
"int curr_x0 = ( ( global_y * TILE_M + 0 ) % output_width ) * STRIDE_X;\n"
"int curr_x1 = ( ( global_y * TILE_M + 1 ) % output_width ) * STRIDE_X;\n"
"int curr_y0 = ( ( global_y * TILE_M + 0 ) / output_width ) * STRIDE_Y;\n"
"int curr_y1 = ( ( global_y * TILE_M + 1 ) / output_width ) * STRIDE_Y;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"int saved_y0 = curr_y0;\n"
"int saved_y1 = curr_y1;\n"
"#endif\n"
"const __global Dtype *src0_read0 = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y0 - INPUT_PAD_H) * ROW_PITCH\n"
"+ curr_x0 - INPUT_PAD_W;\n"
"const __global Dtype *src0_read1 = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y1 - INPUT_PAD_H) * ROW_PITCH\n"
"+ curr_x1 - INPUT_PAD_W;\n"
"const __global Dtype *src1_read = src1 + ( global_x * TILE_N * 2);\n"
"int patch_depth = 0;\n"
"do\n"
"{\n"
"int patch_row = 0;\n"
"do\n"
"{\n"
"const bool kernel_width_is_odd = KERNEL_WIDTH % 2 == 1;\n"
"#if INPUT_PAD_H == 0 && INPUT_PAD_W == 0 && DILATION_X == 1 && DILATION_Y == 1\n"
"Dtype_t blockA00 = ( (const __global Dtype_t*)src0_read0 )[  0  ]; src0_read0 += ROW_PITCH;\n"
"Dtype_t blockA01 = ( (const __global Dtype_t*)src0_read1 )[  0  ]; src0_read1 += ROW_PITCH;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"Dtype*  pblockA01 = (Dtype*)(&blockA01);\n"
"#else\n"
"Dtype_t blockA00;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"int pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y0 >= INPUT_PAD_H && curr_y0 < input_height + INPUT_PAD_H && curr_x0 + pos * DILATION_X >= INPUT_PAD_W && curr_x0 + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA00[pos] = src0_read0[pos * DILATION_X];\n"
"else\n"
"pblockA00[pos] = 0;\n"
"})\n"
"curr_y0 += DILATION_Y;\n"
"Dtype_t blockA01;\n"
"Dtype*  pblockA01 = (Dtype*)(&blockA01);\n"
"pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y1 >= INPUT_PAD_H && curr_y1 < input_height + INPUT_PAD_H && curr_x1 + pos * DILATION_X >= INPUT_PAD_W && curr_x1 + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA01[pos] = src0_read1[pos * DILATION_X];\n"
"else\n"
"pblockA01[pos] = 0;\n"
"})\n"
"curr_y1 += DILATION_Y;\n"
"src0_read0 += (ROW_PITCH * DILATION_Y);\n"
"src0_read1 += (ROW_PITCH * DILATION_Y);\n"
"#endif\n"
"Dtype blockB00[KERNEL_WIDTH*4];\n"
"Dtype8* p8BlockB00 = (Dtype8*)blockB00;\n"
"Dtype4* p4BlockB00 = (Dtype4*)blockB00;\n"
"Dtype*  pBlockB00 =  (Dtype* )blockB00;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"p8BlockB00[interleaved_y] = as_Dtype8( SUB_GROUP_BLOCK_READ8( (const __global INT_TYPE*)src1_read ) );\n"
"src1_read += WIDTH1 * 2;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"p4BlockB00[KERNEL_WIDTH - 1] = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read ) );\n"
"src1_read += WIDTH1 * 2;\n"
"}\n"
"kernel_idx = 0;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC01, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC01, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC11, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC11, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC21, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC21, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC31, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC31, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_8( blockC00, pblockA00[kernel_y], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC01, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC10, pblockA00[kernel_y], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC11, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC20, pblockA00[kernel_y], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC21, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC30, pblockA00[kernel_y], pBlockB00[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC31, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"}\n"
"}\n"
"while( ++patch_row < KERNEL_HEIGHT );\n"
"#if INPUT_PAD_W != 0 || INPUT_PAD_H != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"curr_y0 = saved_y0;\n"
"curr_y1 = saved_y1;\n"
"#endif\n"
"src0_read0 += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"src0_read1 += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"}\n"
"while ( ++patch_depth < INPUT_DEPTH );\n"
"int out0_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M + 0 ) / output_width + OUT_PADDING_HEIGHT ) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M + 0 ) % output_width ) + OUT_PADDING_LEFT;\n"
"int out1_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M + 1 ) / output_width + OUT_PADDING_HEIGHT ) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M + 1 ) % output_width ) + OUT_PADDING_LEFT;\n"
"#if APPLY_BIAS\n"
"Dtype bias[4];\n"
"Dtype4 *bias_vec;\n"
"bias_vec = (Dtype4*)bias;\n"
"*bias_vec = as_Dtype4(SUB_GROUP_BLOCK_READ4((__global INT_TYPE *)biases_base + group_x * TILE_N));\n"
"#endif\n"
"if( global_y * TILE_M < output_width * output_height )\n"
"{\n"
"for( int i = 0; i < 8; i++ )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out0_offset + ( 0+i) * out_pitch_y, blockC00[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"ACTIVATION_FUNCTION(dst, out0_offset + ( 8+i) * out_pitch_y, blockC10[i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 8);\n"
"ACTIVATION_FUNCTION(dst, out0_offset + (16+i) * out_pitch_y, blockC20[i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + i + 16);\n"
"ACTIVATION_FUNCTION(dst, out0_offset + (24+i) * out_pitch_y, blockC30[i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + i + 24);\n"
"}\n"
"}\n"
"if( global_y * TILE_M + 1 < output_width * output_height )\n"
"{\n"
"for( int i = 0; i < 8; i++ )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out1_offset + ( 0+i) * out_pitch_y, blockC01[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"ACTIVATION_FUNCTION(dst, out1_offset + ( 8+i) * out_pitch_y, blockC11[i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 8);\n"
"ACTIVATION_FUNCTION(dst, out1_offset + (16+i) * out_pitch_y, blockC21[i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + i + 16);\n"
"ACTIVATION_FUNCTION(dst, out1_offset + (24+i) * out_pitch_y, blockC31[i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + i + 24);\n"
"}\n"
"}\n"
"}\n"
"#if TILE_N_LAST > 0\n"
"else\n"
"{\n"
"int i = 0;\n"
"Dtype8  blockC0[TILE_N_LAST_DIV8];\n"
"Dtype8  blockC1[TILE_N_LAST_DIV8];\n"
"LOOP(TILE_N_LAST_DIV8, i,\n"
"{\n"
"blockC0[i] = 0.f;\n"
"blockC1[i] = 0.f;\n"
"} )\n"
"int curr_x0 = ( ( global_y * TILE_M + 0 ) % output_width ) * STRIDE_X;\n"
"int curr_x1 = ( ( global_y * TILE_M + 1 ) % output_width ) * STRIDE_X;\n"
"int curr_y0 = ( ( global_y * TILE_M + 0 ) / output_width ) * STRIDE_Y;\n"
"int curr_y1 = ( ( global_y * TILE_M + 1 ) / output_width ) * STRIDE_Y;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"int saved_y0 = curr_y0;\n"
"int saved_y1 = curr_y1;\n"
"#endif\n"
"const __global Dtype *src0_read0 = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y0 - INPUT_PAD_H) * ROW_PITCH\n"
"+ curr_x0 - INPUT_PAD_W;\n"
"const __global Dtype *src0_read1 = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y1 - INPUT_PAD_H) * ROW_PITCH\n"
"+ curr_x1 - INPUT_PAD_W;\n"
"const __global Dtype *src1_read = src1 + ( global_x * TILE_N  * 2);\n"
"int patch_depth = 0;\n"
"do\n"
"{\n"
"int patch_row = 0;\n"
"do\n"
"{\n"
"const bool kernel_width_is_odd = KERNEL_WIDTH % 2 == 1;\n"
"#if INPUT_PAD_H == 0 && INPUT_PAD_W == 0 && DILATION_X == 1 && DILATION_Y == 1\n"
"Dtype_t blockA00 = ( (const __global Dtype_t*)src0_read0 )[  0  ]; src0_read0 += ROW_PITCH;\n"
"Dtype_t blockA01 = ( (const __global Dtype_t*)src0_read1 )[  0  ]; src0_read1 += ROW_PITCH;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"Dtype*  pblockA01 = (Dtype*)(&blockA01);\n"
"#else\n"
"Dtype_t blockA00;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"int pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y0 >= INPUT_PAD_H && curr_y0 < input_height + INPUT_PAD_H && curr_x0 + pos * DILATION_X >= INPUT_PAD_W && curr_x0 + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA00[pos] = src0_read0[pos * DILATION_X];\n"
"else\n"
"pblockA00[pos] = 0;\n"
"})\n"
"curr_y0 += DILATION_Y;\n"
"Dtype_t blockA01;\n"
"Dtype*  pblockA01 = (Dtype*)(&blockA01);\n"
"pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y1 >= INPUT_PAD_H && curr_y1 < input_height + INPUT_PAD_H && curr_x1 + pos * DILATION_X >= INPUT_PAD_W && curr_x1 + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA01[pos] = src0_read1[pos * DILATION_X];\n"
"else\n"
"pblockA01[pos] = 0;\n"
"})\n"
"curr_y1 += DILATION_Y;\n"
"src0_read0 += (ROW_PITCH * DILATION_Y);\n"
"src0_read1 += (ROW_PITCH * DILATION_Y);\n"
"#endif\n"
"Dtype blockB[KERNEL_WIDTH * TILE_N_LAST_DIV8];\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"#if TILE_N_LAST_DIV8 == 1\n"
"Dtype2* p2BlockB = (Dtype2* )blockB;\n"
"p2BlockB[interleaved_y] = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 2\n"
"Dtype4* p4BlockB = (Dtype4* )blockB;\n"
"p4BlockB[interleaved_y] = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 3\n"
"Dtype6* p6BlockB = (Dtype6* )blockB;\n"
"(*((Dtype8*)(&p6BlockB[interleaved_y]))).s0123 = as_Dtype4( SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read ) );\n"
"(*((Dtype8*)(&p6BlockB[interleaved_y]))).s45 = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)(src1_read + 4 * 8) ) );\n"
"#endif\n"
"src1_read += WIDTH1 * 2;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"#if TILE_N_LAST_DIV8 == 1\n"
"Dtype* pBlockB = (Dtype* )blockB;\n"
"pBlockB[KERNEL_WIDTH - 1] = as_Dtype( SUB_GROUP_BLOCK_READ( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 2\n"
"Dtype2* p2BlockB = (Dtype2* )blockB;\n"
"p2BlockB[KERNEL_WIDTH - 1] = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"#elif TILE_N_LAST_DIV8 == 3\n"
"Dtype3* p3BlockB = (Dtype3* )blockB;\n"
"p3BlockB[KERNEL_WIDTH - 1].s01 = as_Dtype2( SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read ) );\n"
"p3BlockB[KERNEL_WIDTH - 1].s2 = as_Dtype( SUB_GROUP_BLOCK_READ( (const __global INT_TYPE*) (src1_read + 8) ) );\n"
"#endif\n"
"src1_read += WIDTH1 * 2;\n"
"}\n"
"Dtype* pBlockB = (Dtype*)blockB;\n"
"kernel_idx = 0;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_8( blockC0[0], pblockA00[kernel_y    ], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[0], pblockA01[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC0[0], pblockA00[kernel_y + 1], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[0], pblockA01[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 2\n"
"DOT_PRODUCT_8( blockC0[1], pblockA00[kernel_y    ], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[1], pblockA01[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC0[1], pblockA00[kernel_y + 1], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[1], pblockA01[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 3\n"
"DOT_PRODUCT_8( blockC0[2], pblockA00[kernel_y    ], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[2], pblockA01[kernel_y    ], pBlockB[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_8( blockC0[2], pblockA00[kernel_y + 1], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[2], pblockA01[kernel_y + 1], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#endif\n"
"#endif\n"
"} )\n"
"kernel_y = interleaved_y * 2;\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"DOT_PRODUCT_8( blockC0[0], pblockA00[kernel_y], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[0], pblockA01[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 2\n"
"DOT_PRODUCT_8( blockC0[1], pblockA00[kernel_y], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[1], pblockA01[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#if TILE_N_LAST_DIV8 >= 3\n"
"DOT_PRODUCT_8( blockC0[2], pblockA00[kernel_y], pBlockB[kernel_idx] );\n"
"DOT_PRODUCT_8( blockC1[2], pblockA01[kernel_y], pBlockB[kernel_idx] ); kernel_idx++;\n"
"#endif\n"
"#endif\n"
"}\n"
"}\n"
"while( ++patch_row < KERNEL_HEIGHT );\n"
"#if INPUT_PAD_W != 0 || INPUT_PAD_H != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"curr_y0 = saved_y0;\n"
"curr_y1 = saved_y1;\n"
"#endif\n"
"src0_read0 += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"src0_read1 += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"}\n"
"while ( ++patch_depth < INPUT_DEPTH );\n"
"int out0_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M + 0 ) / output_width + OUT_PADDING_HEIGHT ) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M + 0 ) % output_width ) + OUT_PADDING_LEFT;\n"
"int out1_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M + 1 ) / output_width + OUT_PADDING_HEIGHT ) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M + 1 ) % output_width ) + OUT_PADDING_LEFT;\n"
"__global Dtype *out1 = dst + out1_offset;\n"
"#if APPLY_BIAS\n"
"Dtype bias[4];\n"
"Dtype4 *bias_vec;\n"
"bias_vec = (Dtype4*)bias;\n"
"*bias_vec = as_Dtype4(SUB_GROUP_BLOCK_READ4((__global INT_TYPE *)biases_base + group_x * TILE_N));\n"
"#endif\n"
"if( global_y * TILE_M < output_width * output_height )\n"
"{\n"
"for( int i = 0; i < 8; i++ )\n"
"{\n"
"if ( TILE_N_LAST_DIV8 > 0 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out0_offset + ( 0+i) * out_pitch_y, blockC0[0][i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 1 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out0_offset + ( 8+i) * out_pitch_y, blockC0[1][i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 8);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 2 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out0_offset + (16+i) * out_pitch_y, blockC0[2][i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + i + 16);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 3 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out0_offset + (24+i) * out_pitch_y, blockC0[3][i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + i + 24);\n"
"}\n"
"}\n"
"}\n"
"if( global_y * TILE_M + 1 < output_width * output_height )\n"
"{\n"
"for( int i = 0; i < 8; i++ )\n"
"{\n"
"if ( TILE_N_LAST_DIV8 > 0 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out1_offset + ( 0+i) * out_pitch_y, blockC1[0][i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 1 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out1_offset + ( 8+i) * out_pitch_y, blockC1[1][i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 8);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 2 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out1_offset + (16+i) * out_pitch_y, blockC1[2][i] + SUBGROUP_GET_BIAS(2, i), group_x * TILE_N + i + 16);\n"
"}\n"
"if ( TILE_N_LAST_DIV8 > 3 )\n"
"{\n"
"ACTIVATION_FUNCTION(dst, out1_offset + (24+i) * out_pitch_y, blockC1[3][i] + SUBGROUP_GET_BIAS(3, i), group_x * TILE_N + i + 24);\n"
"}\n"
"}\n"
"}\n"
"}\n"
"#endif\n"
"}\n"
"#endif\n"
"#if defined(GEMM_LIKE_CONV_32_2_SIMD16) || defined(GEMM_LIKE_CONV_32_1_SIMD16)\n"
"#define INTERLEAVED_SIMD16_OUTPUT(_out_, _offset_,  _m_) do {\\\n"
"if (global_y * TILE_M < output_width * output_height ) \\\n"
"{ \\\n"
"if ( ( OUT_DEPTH % TILE_N ) == 0 ) {\\\n"
"for (int i = 0; i < 16; i++) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_ [i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + (16+i) * out_pitch_y, blockC1 ##_m_ [i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 16); \\\n"
"} \\\n"
"} \\\n"
"else if( ( OUT_DEPTH % 16 ) == 0 ) { \\\n"
"if ( ( global_x + 1 ) < get_global_size(0) ) { \\\n"
"for ( int i = 0; i < 16; i++ ) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_ [i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + (16+i) * out_pitch_y, blockC1 ##_m_ [i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 16); \\\n"
"} \\\n"
"} \\\n"
"else { \\\n"
"for (int i = 0; i < 16; i++) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_ [i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"} \\\n"
"} \\\n"
"} \\\n"
"else { \\\n"
"if ( ( global_x + 1 ) < get_global_size(0) ) \\\n"
"{ \\\n"
"for ( int i = 0; i < 16; i++ ) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + (16+i) * out_pitch_y, blockC1 ##_m_[i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 16); \\\n"
"} \\\n"
"} \\\n"
"else { \\\n"
"if ( (OUT_DEPTH % TILE_N) > 16 ) { \\\n"
"for (int i = 0; i < 16 ; i++) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"} \\\n"
"for (int i = 0; i < OUT_DEPTH % 16 ; i++) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + (16+i) * out_pitch_y, blockC1 ##_m_[i] + SUBGROUP_GET_BIAS(1, i), group_x * TILE_N + i + 16); \\\n"
"} \\\n"
"} \\\n"
"else { \\\n"
"for (int i = 0; i < OUT_DEPTH % 16 ; i++) \\\n"
"{ \\\n"
"ACTIVATION_FUNCTION(_out_, _offset_ + ( 0+i) * out_pitch_y, blockC0 ##_m_[i] + SUBGROUP_GET_BIAS(0, i), group_x * TILE_N + i); \\\n"
"} \\\n"
"} \\\n"
"} \\\n"
"} \\\n"
"} \\\n"
"}while(0)\n"
"#endif\n"
"#ifdef GEMM_LIKE_CONV_32_1_SIMD16\n"
"#define TILE_M          1\n"
"#define TILE_K          KERNEL_WIDTH\n"
"#define TILE_N          32\n"
"#ifndef __BEIGNET__\n"
"__attribute__((intel_reqd_sub_group_size(16)))\n"
"#endif\n"
"__kernel void Conv_Interleaved(GEMM_LIKE_KERNEL_ARGS)\n"
"{\n"
"const int group_x = get_group_id(0);\n"
"const int group_y = get_group_id(1);\n"
"const int global_x = get_global_id(0);\n"
"const int global_y = get_global_id(1);\n"
"const int global_z = get_global_id(2);\n"
"int interleaved_y;\n"
"int kernel_y;\n"
"int kernel_idx;\n"
"Dtype16  blockC00 = 0.f;\n"
"Dtype16  blockC10 = 0.f;\n"
"int curr_x = ( global_y % output_width ) * STRIDE_X;\n"
"int curr_y = ( global_y / output_width ) * STRIDE_Y;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"int saved_y = curr_y;\n"
"#endif\n"
"const __global Dtype *src0_read = src0\n"
"+ aligned_input_size * global_z\n"
"+ (curr_y - INPUT_PAD_H) * ROW_PITCH\n"
"+ curr_x - INPUT_PAD_W;\n"
"const __global Dtype *src0_read_orig = src0_read;\n"
"const __global Dtype *src1_read = src1 + ( global_x * TILE_N * 2 );\n"
"#define DOT_PRODUCT_16( _result, _rowA, colB )    \\\n"
"{   \\\n"
"_result.s0 = mad( _rowA, sub_group_broadcast( colB,  0 ), _result.s0 );  \\\n"
"_result.s1 = mad( _rowA, sub_group_broadcast( colB,  1 ), _result.s1 );  \\\n"
"_result.s2 = mad( _rowA, sub_group_broadcast( colB,  2 ), _result.s2 );  \\\n"
"_result.s3 = mad( _rowA, sub_group_broadcast( colB,  3 ), _result.s3 );  \\\n"
"_result.s4 = mad( _rowA, sub_group_broadcast( colB,  4 ), _result.s4 );  \\\n"
"_result.s5 = mad( _rowA, sub_group_broadcast( colB,  5 ), _result.s5 );  \\\n"
"_result.s6 = mad( _rowA, sub_group_broadcast( colB,  6 ), _result.s6 );  \\\n"
"_result.s7 = mad( _rowA, sub_group_broadcast( colB,  7 ), _result.s7 );  \\\n"
"_result.s8 = mad( _rowA, sub_group_broadcast( colB,  8 ), _result.s8 );  \\\n"
"_result.s9 = mad( _rowA, sub_group_broadcast( colB,  9 ), _result.s9 );  \\\n"
"_result.sa = mad( _rowA, sub_group_broadcast( colB, 10 ), _result.sa );  \\\n"
"_result.sb = mad( _rowA, sub_group_broadcast( colB, 11 ), _result.sb );  \\\n"
"_result.sc = mad( _rowA, sub_group_broadcast( colB, 12 ), _result.sc );  \\\n"
"_result.sd = mad( _rowA, sub_group_broadcast( colB, 13 ), _result.sd );  \\\n"
"_result.se = mad( _rowA, sub_group_broadcast( colB, 14 ), _result.se );  \\\n"
"_result.sf = mad( _rowA, sub_group_broadcast( colB, 15 ), _result.sf );  \\\n"
"}\n"
"typedef CAT( Dtype, KERNEL_WIDTH ) Dtype_t;\n"
"int patch_depth = 0;\n"
"#ifndef __BEIGNET__\n"
"__attribute__((opencl_unroll_hint(1)))\n"
"#endif\n"
"do\n"
"{\n"
"int patch_row = 0;\n"
"#if INPUT_PAD_H != 0 || INPUT_PAD_W != 0 || DILATION_X != 1 || DILATION_Y != 1\n"
"curr_y = saved_y;\n"
"#endif\n"
"#ifndef __BEIGNET__\n"
"__attribute__((opencl_unroll_hint(1)))\n"
"#endif\n"
"do\n"
"{\n"
"const bool kernel_width_is_odd = KERNEL_WIDTH % 2 == 1;\n"
"#if INPUT_PAD_W == 0 && INPUT_PAD_H == 0 && DILATION_X == 1 && DILATION_Y == 1\n"
"Dtype_t blockA00 = ( (const __global Dtype_t*)src0_read )[  0  ];\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"#else\n"
"Dtype_t blockA00;\n"
"Dtype*  pblockA00 = (Dtype*)(&blockA00);\n"
"int pos = 0;\n"
"LOOP(KERNEL_WIDTH, pos,\n"
"{\n"
"if (curr_y >= INPUT_PAD_H && curr_y < input_height + INPUT_PAD_H && curr_x + pos * DILATION_X >= INPUT_PAD_W && curr_x + pos * DILATION_X < input_width + INPUT_PAD_W)\n"
"pblockA00[pos] = src0_read[pos * DILATION_X];\n"
"else\n"
"pblockA00[pos] = 0;\n"
"})\n"
"curr_y += DILATION_Y;\n"
"#endif\n"
"src0_read += ROW_PITCH * DILATION_Y;\n"
"INT_TYPE blockB00[KERNEL_WIDTH * 2];\n"
"INT_TYPE4* p4BlockB00 = (INT_TYPE4*)blockB00;\n"
"INT_TYPE2* p2BlockB00 = (INT_TYPE2*)blockB00;\n"
"Dtype* pBlockB00  = (Dtype*)blockB00;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"p4BlockB00[interleaved_y] = SUB_GROUP_BLOCK_READ4( (const __global INT_TYPE*)src1_read );\n"
"src1_read += WIDTH1 * 2;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"p2BlockB00[KERNEL_WIDTH - 1] = SUB_GROUP_BLOCK_READ2( (const __global INT_TYPE*)src1_read );\n"
"src1_read += WIDTH1 * 2;\n"
"}\n"
"kernel_idx = 0;\n"
"interleaved_y = 0;\n"
"LOOP(KERNEL_WIDTH_DIV2, interleaved_y,\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_16( blockC00, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_16( blockC00, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_16( blockC10, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_16( blockC10, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"} )\n"
"if ( kernel_width_is_odd )\n"
"{\n"
"kernel_y = interleaved_y * 2;\n"
"DOT_PRODUCT_16( blockC00, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"DOT_PRODUCT_16( blockC10, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;\n"
"}\n"
"}\n"
"while( ++patch_row < KERNEL_HEIGHT );\n"
"src0_read += slice_pitch - ( KERNEL_HEIGHT * ROW_PITCH * DILATION_Y );\n"
"}\n"
"while ( ++patch_depth < INPUT_DEPTH );\n"
"int out_offset = global_z * out_pitch_z\n"
"+ ( group_x * TILE_N ) * out_pitch_y\n"
"+ ( ( global_y * TILE_M ) / output_width + OUT_PADDING_HEIGHT) * OUT_PITCH_X\n"
"+ ( ( global_y * TILE_M ) % output_width ) + OUT_PADDING_LEFT;\n"
"__global Dtype *out = dst + out_offset;\n"
"#if APPLY_BIAS\n"
"Dtype bias[2];\n"
"Dtype2 *bias_vec;\n"
"bias_vec = (Dtype2*)bias;\n"
"*bias_vec = as_Dtype2(SUB_GROUP_BLOCK_READ2((__global INT_TYPE *)biases_base + group_x * TILE_N));\n"
"#endif\n"
"INTERLEAVED_SIMD16_OUTPUT(dst, out_offset, 0);\n"
"}\n"
"#endif\n"
"#endif\n"
, "ccb170084eca3f37daeda06b73f6998f", NULL};
struct cv::ocl::internal::ProgramEntry conv_spatial_helper_oclsrc={moduleName, "conv_spatial_helper",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"__kernel void TEMPLATE(copyWeightsSwizzled, Dtype)\n"
"(__global Dtype* weightIn,\n"
"__global Dtype* weightOut,\n"
"const int kernel_w,\n"
"const int kernel_h,\n"
"const int channels,\n"
"const int outputs,\n"
"const int swizzleFactor) {\n"
"unsigned int sX = get_global_id(0);\n"
"int outputSublayer = channels / swizzleFactor;\n"
"int outputSublayerIndex = channels % swizzleFactor;\n"
"int filter = sX / (kernel_w*kernel_h*channels);\n"
"int kernel_X = sX % kernel_w;\n"
"int kernel_Y = (sX / kernel_w) % kernel_h;\n"
"int kernel_C = (sX / (kernel_w * kernel_h)) % channels;\n"
"int FP = filter / swizzleFactor;\n"
"int F1 = filter % swizzleFactor;\n"
"weightOut[FP*(kernel_w*kernel_h*channels*swizzleFactor) + kernel_C*(kernel_w*kernel_h*swizzleFactor) + kernel_Y*(kernel_w*swizzleFactor) + kernel_X*swizzleFactor + F1]\n"
"= weightIn[filter*(kernel_w*kernel_h*channels) + kernel_C*(kernel_w*kernel_h) + kernel_Y*kernel_w + kernel_X];\n"
"}\n"
, "c67391f875fb5136ef861527e047e7fb", NULL};
struct cv::ocl::internal::ProgramEntry dummy_oclsrc={moduleName, "dummy",
"__kernel void dummy_kernel()\n"
"{\n"
"}\n"
, "697bd1a0f09685d066b8946e159d42bc", NULL};
struct cv::ocl::internal::ProgramEntry gemm_image_oclsrc={moduleName, "gemm_image",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define int_tp int\n"
"#define uint_tp unsigned int\n"
"#define Dtype  float\n"
"#define Dtype2 float2\n"
"#define Dtype4 float4\n"
"#define Dtype8 float8\n"
"#define as_Dtype  as_float\n"
"#define as_Dtype2 as_float2\n"
"#define as_Dtype4 as_float4\n"
"#define as_Dtype8 as_float8\n"
"#define KERNEL_ARG_DTYPE float\n"
"#if defined(cl_intel_subgroups)\n"
"#pragma OPENCL EXTENSION  cl_intel_subgroups : enable\n"
"#endif\n"
"#define TILE_M          32\n"
"#define TILE_K          8\n"
"#define SUBGROUP_BLOCK_READ8( __image, __coord ) intel_sub_group_block_read8( __image, __coord )\n"
"#define SHUFFLE_TYPE2(val) val\n"
"#define SHUFFLE_TYPE8(val) val\n"
"#define READ_IMAGE(__image, __coord) read_imagef(__image, sampler, __coord)\n"
"#define SIZE_OF_ELEMENT sizeof(uint)\n"
"#define SIMD_SIZE_GEMM 8\n"
"#define TILE_N 8\n"
"#ifdef USE_IMAGE_C\n"
"#define BLOCKC_READ8( _C, _coordC ) as_Dtype8( intel_sub_group_block_read8( _C, _coordC ) )\n"
"#define BLOCKC_WRITE8( _C, _coordC, _val ) intel_sub_group_block_write8( _C, _coordC, as_uint8( _val ) )\n"
"#define MATC_PARAMETER __read_only image2d_t C, __write_only image2d_t dst\n"
"#define GEMM_OUTPUT(ALPHA1, BETA_NOT0) GEMM_OUTPUT_EXT(ALPHA1, BETA_NOT0, C, dst, sizeof(uint))\n"
"#else\n"
"#define BLOCKC_READ8( _C, _coordC ) \\\n"
"(Dtype8) ( (_coordC.x + get_local_id(0) < N && _coordC.y < M) ? _C[ _coordC.y * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 1 < M) ? _C[ ( _coordC.y + 1 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 2 < M) ? _C[ ( _coordC.y + 2 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 3 < M) ? _C[ ( _coordC.y + 3 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 4 < M) ? _C[ ( _coordC.y + 4 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 5 < M) ? _C[ ( _coordC.y + 5 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 6 < M) ? _C[ ( _coordC.y + 6 ) * ldc + _coordC.x + get_local_id(0) ] : 0, \\\n"
"(_coordC.x + get_local_id(0) < N && _coordC.y + 7 < M) ? _C[ ( _coordC.y + 7 ) * ldc + _coordC.x + get_local_id(0) ] : 0)\n"
"#define BLOCKC_WRITE8( _C, _coordC, _val) do {\\\n"
"if (_coordC.x + get_local_id(0) < N) { \\\n"
"if (_coordC.y < M) \\\n"
"_C[ _coordC.y * ldc + _coordC.x + get_local_id(0) ] = _val.s0; \\\n"
"if (_coordC.y + 1 < M) \\\n"
"_C[ ( _coordC.y + 1 )* ldc + _coordC.x + get_local_id(0) ] = _val.s1; \\\n"
"if (_coordC.y + 2 < M) \\\n"
"_C[ ( _coordC.y + 2 )* ldc + _coordC.x + get_local_id(0) ] = _val.s2; \\\n"
"if (_coordC.y + 3 < M) \\\n"
"_C[ ( _coordC.y + 3 )* ldc + _coordC.x + get_local_id(0) ] = _val.s3; \\\n"
"if (_coordC.y + 4 < M) \\\n"
"_C[ ( _coordC.y + 4 )* ldc + _coordC.x + get_local_id(0) ] = _val.s4; \\\n"
"if (_coordC.y + 5 < M) \\\n"
"_C[ ( _coordC.y + 5 )* ldc + _coordC.x + get_local_id(0) ] = _val.s5; \\\n"
"if (_coordC.y + 6 < M) \\\n"
"_C[ ( _coordC.y + 6 )* ldc + _coordC.x + get_local_id(0) ] = _val.s6; \\\n"
"if (_coordC.y + 7 < M) \\\n"
"_C[ ( _coordC.y + 7 )* ldc + _coordC.x + get_local_id(0) ] = _val.s7; \\\n"
"}} while(0)\n"
"#define MATC_PARAMETER __global Dtype * C, const int offC, const int M, const int N, const int ldc\n"
"#define GEMM_OUTPUT(ALPHA1, BETA_NOT0) GEMM_OUTPUT_EXT(ALPHA1, BETA_NOT0, (C + offC), (C + offC), 1)\n"
"#endif\n"
"#define GEMM_OUTPUT_EXT(ALPHA1, BETA_NOT0, _C, _dst, _C_step) \\\n"
"int2    coordDst = (int2)( ( group_x * TILE_N ) * _C_step, ( group_y * TILE_M ) ); \\\n"
"int2    coordC = coordDst; \\\n"
"Dtype8 blockC00; \\\n"
"Dtype8 blockC01; \\\n"
"Dtype8 blockC02; \\\n"
"Dtype8 blockC03; \\\n"
"if (BETA_NOT0) { \\\n"
"blockC00 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC01 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC02 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC03 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC ); \\\n"
"if (!ALPHA1) { \\\n"
"blockC00 = mad(blockAxB00, (Dtype8)alpha, blockC00); \\\n"
"blockC01 = mad(blockAxB01, (Dtype8)alpha, blockC01); \\\n"
"blockC02 = mad(blockAxB02, (Dtype8)alpha, blockC02); \\\n"
"blockC03 = mad(blockAxB03, (Dtype8)alpha, blockC03); \\\n"
"} else { \\\n"
"blockC00 += blockAxB00; \\\n"
"blockC01 += blockAxB01; \\\n"
"blockC02 += blockAxB02; \\\n"
"blockC03 += blockAxB03; \\\n"
"} \\\n"
"} else { \\\n"
"blockC00 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC01 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC02 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC );    coordC.y += 8; \\\n"
"blockC03 = isFirstColBlock ? BLOCKC_READ8( _C, coordC ) * beta : BLOCKC_READ8( _C, coordC ); \\\n"
"if (!ALPHA1) { \\\n"
"blockC00 = mad(blockAxB00, (Dtype8)alpha, blockC00); \\\n"
"blockC01 = mad(blockAxB01, (Dtype8)alpha, blockC01); \\\n"
"blockC02 = mad(blockAxB02, (Dtype8)alpha, blockC02); \\\n"
"blockC03 = mad(blockAxB03, (Dtype8)alpha, blockC03); \\\n"
"} else { \\\n"
"blockC00 += blockAxB00; \\\n"
"blockC01 += blockAxB01; \\\n"
"blockC02 += blockAxB02; \\\n"
"blockC03 += blockAxB03; \\\n"
"} \\\n"
"} \\\n"
"BLOCKC_WRITE8( _dst, coordDst, blockC00 );    coordDst.y += 8; \\\n"
"BLOCKC_WRITE8( _dst, coordDst, blockC01 );    coordDst.y += 8; \\\n"
"BLOCKC_WRITE8( _dst, coordDst, blockC02 );    coordDst.y += 8; \\\n"
"BLOCKC_WRITE8( _dst, coordDst, blockC03 );\n"
"#define TRANSPOSE_BLOCK_8( _block, _col )   \\\n"
"(Dtype8)( intel_sub_group_shuffle( _block.s0, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s1, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s2, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s3, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s4, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s5, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s6, _col ),   \\\n"
"intel_sub_group_shuffle( _block.s7, _col ) );\n"
"#define MULTIPLY_BLOCKS_8x8( _result, _blockA, _blockB )    \\\n"
"{   \\\n"
"const Dtype8    acol0 = TRANSPOSE_BLOCK_8( _blockA, 0 );    \\\n"
"const Dtype8    acol1 = TRANSPOSE_BLOCK_8( _blockA, 1 );    \\\n"
"const Dtype8    acol2 = TRANSPOSE_BLOCK_8( _blockA, 2 );    \\\n"
"const Dtype8    acol3 = TRANSPOSE_BLOCK_8( _blockA, 3 );    \\\n"
"const Dtype8    acol4 = TRANSPOSE_BLOCK_8( _blockA, 4 );    \\\n"
"const Dtype8    acol5 = TRANSPOSE_BLOCK_8( _blockA, 5 );    \\\n"
"const Dtype8    acol6 = TRANSPOSE_BLOCK_8( _blockA, 6 );    \\\n"
"const Dtype8    acol7 = TRANSPOSE_BLOCK_8( _blockA, 7 );    \\\n"
"_result = mad( (Dtype8)(_blockB.s0), acol0, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s1), acol1, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s2), acol2, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s3), acol3, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s4), acol4, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s5), acol5, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s6), acol6, _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s7), acol7, _result );      \\\n"
"}\n"
"#define GEMM_NN(ALPHA1, BETA_NOT0) \\\n"
"__attribute__((intel_reqd_sub_group_size(SIMD_SIZE_GEMM))) \\\n"
"__attribute__((reqd_work_group_size(SIMD_SIZE_GEMM, 1, 1))) \\\n"
"__kernel void TEMPLATE(gemm_32_1_NN_ ##ALPHA1 ##_ ##BETA_NOT0, Dtype)( \\\n"
"__read_only image2d_t A, \\\n"
"__read_only image2d_t B, \\\n"
"MATC_PARAMETER, \\\n"
"KERNEL_ARG_DTYPE alpha_in, \\\n"
"KERNEL_ARG_DTYPE beta_in, \\\n"
"int width0, \\\n"
"int isFirstColBlock) \\\n"
"{ \\\n"
"const Dtype alpha = (Dtype)alpha_in; \\\n"
"const Dtype beta = (Dtype)beta_in; \\\n"
"const int group_x = get_group_id(0); \\\n"
"const int group_y = get_group_id(1); \\\n"
"Dtype8 blockAxB00 = 0.0f; \\\n"
"Dtype8 blockAxB01 = 0.0f; \\\n"
"Dtype8 blockAxB02 = 0.0f; \\\n"
"Dtype8 blockAxB03 = 0.0f; \\\n"
"int2    coordA = (int2)( 0, group_y * TILE_M ); \\\n"
"int2    coordB = (int2)( ( group_x * TILE_N ) * SIZE_OF_ELEMENT, 0 ); \\\n"
"do \\\n"
"{  \\\n"
"int2    coordBTemp = coordB; \\\n"
"Dtype8  blockB00 = as_Dtype8( SUBGROUP_BLOCK_READ8( B, coordBTemp ) );    coordB.y += TILE_K; \\\n"
"int2    coordATemp = coordA; \\\n"
"Dtype8  blockA00 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8  blockA01 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8  blockA02 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8  blockA03 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordA.x += TILE_K * SIZE_OF_ELEMENT; \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB00, blockA00, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB01, blockA01, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB02, blockA02, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB03, blockA03, blockB00 ); \\\n"
"} \\\n"
"while( coordB.y < width0 ); \\\n"
"GEMM_OUTPUT(ALPHA1, BETA_NOT0); \\\n"
"}\n"
"GEMM_NN(1, 0)\n"
"GEMM_NN(1, 1)\n"
"GEMM_NN(0, 0)\n"
"GEMM_NN(0, 1)\n"
"#undef TRANSPOSE_BLOCK_8\n"
"#undef MULTIPLY_BLOCKS_8x8\n"
"#undef GEMM_NN\n"
"#define TRANSPOSE_BLOCK_8(_vec, _col) \\\n"
"(Dtype8)( intel_sub_group_shuffle(_vec, _col + 0), \\\n"
"intel_sub_group_shuffle(_vec, _col + 1), \\\n"
"intel_sub_group_shuffle(_vec, _col + 2), \\\n"
"intel_sub_group_shuffle(_vec, _col + 3), \\\n"
"intel_sub_group_shuffle(_vec, _col + 4), \\\n"
"intel_sub_group_shuffle(_vec, _col + 5), \\\n"
"intel_sub_group_shuffle(_vec, _col + 6), \\\n"
"intel_sub_group_shuffle(_vec, _col + 7) )\n"
"#define MULTIPLY_BLOCKS_8x8( _result, _blockA, _blockB, _col )    \\\n"
"{   \\\n"
"_result = mad( (Dtype8)(_blockB.s0), TRANSPOSE_BLOCK_8(_blockA.s0, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s1), TRANSPOSE_BLOCK_8(_blockA.s1, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s2), TRANSPOSE_BLOCK_8(_blockA.s2, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s3), TRANSPOSE_BLOCK_8(_blockA.s3, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s4), TRANSPOSE_BLOCK_8(_blockA.s4, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s5), TRANSPOSE_BLOCK_8(_blockA.s5, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s6), TRANSPOSE_BLOCK_8(_blockA.s6, _col), _result );      \\\n"
"_result = mad( (Dtype8)(_blockB.s7), TRANSPOSE_BLOCK_8(_blockA.s7, _col), _result );      \\\n"
"}\n"
"#define GEMM_TN(ALPHA1, BETA_NOT0) \\\n"
"__attribute__((intel_reqd_sub_group_size(SIMD_SIZE_GEMM))) \\\n"
"__attribute__((reqd_work_group_size(SIMD_SIZE_GEMM, 1, 1))) \\\n"
"__kernel void TEMPLATE(gemm_32_1_TN_ ##ALPHA1 ##_ ##BETA_NOT0,Dtype)( \\\n"
"__read_only image2d_t A, \\\n"
"__read_only image2d_t B, \\\n"
"MATC_PARAMETER, \\\n"
"KERNEL_ARG_DTYPE alpha_in, \\\n"
"KERNEL_ARG_DTYPE beta_in, \\\n"
"int width0, \\\n"
"int isFirstColBlock) \\\n"
"{ \\\n"
"const Dtype alpha = (Dtype)alpha_in; \\\n"
"const Dtype beta = (Dtype)beta_in; \\\n"
"const int group_x = get_group_id(0);\\\n"
"const int group_y = get_group_id(1);\\\n"
"Dtype8 blockAxB00 = 0.0f;\\\n"
"Dtype8 blockAxB01 = 0.0f;\\\n"
"Dtype8 blockAxB02 = 0.0f;\\\n"
"Dtype8 blockAxB03 = 0.0f;\\\n"
"int2    coordA = (int2)( group_y * TILE_M * SIZE_OF_ELEMENT, 0 );\\\n"
"int2    coordB = (int2)( ( group_x * TILE_N ) * SIZE_OF_ELEMENT, 0 );\\\n"
"do\\\n"
"{\\\n"
"int2    coordBTemp = coordB;\\\n"
"Dtype8 blockB00 = as_Dtype8( SUBGROUP_BLOCK_READ8( B, coordBTemp ) );    coordB.y += TILE_K;\\\n"
"int2    coordATemp = coordA;\\\n"
"Dtype8 blockA00 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT;\\\n"
"Dtype8 blockA01 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT;\\\n"
"Dtype8 blockA02 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT;\\\n"
"Dtype8 blockA03 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordA.y += TILE_K;\\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB00, blockA00, blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB01, blockA01, blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB02, blockA02, blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB03, blockA03, blockB00, 0 ); \\\n"
"} \\\n"
"while( coordB.y < width0 ); \\\n"
"GEMM_OUTPUT(ALPHA1, BETA_NOT0); \\\n"
"}\n"
"GEMM_TN(1, 0)\n"
"GEMM_TN(1, 1)\n"
"GEMM_TN(0, 0)\n"
"GEMM_TN(0, 1)\n"
"#undef MULTIPLY_BLOCKS_8x8\n"
"#undef TRANSPOSE_BLOCK_8\n"
"#undef GEMM_TN\n"
"#define TRANSPOSE_BLOCK_8( _block, _col )   \\\n"
"(Dtype8)( intel_sub_group_shuffle( _block.s0, _col),   \\\n"
"intel_sub_group_shuffle( _block.s1, _col),   \\\n"
"intel_sub_group_shuffle( _block.s2, _col),   \\\n"
"intel_sub_group_shuffle( _block.s3, _col),   \\\n"
"intel_sub_group_shuffle( _block.s4, _col),   \\\n"
"intel_sub_group_shuffle( _block.s5, _col),   \\\n"
"intel_sub_group_shuffle( _block.s6, _col),   \\\n"
"intel_sub_group_shuffle( _block.s7, _col) )\n"
"#define MULTIPLY_BLOCKS_8x8( _result, _blockA, _blockB )    \\\n"
"{   \\\n"
"const Dtype8    acol0 = TRANSPOSE_BLOCK_8( _blockA, 0 );    \\\n"
"const Dtype8    acol1 = TRANSPOSE_BLOCK_8( _blockA, 1 );    \\\n"
"const Dtype8    acol2 = TRANSPOSE_BLOCK_8( _blockA, 2 );    \\\n"
"const Dtype8    acol3 = TRANSPOSE_BLOCK_8( _blockA, 3 );    \\\n"
"const Dtype8    acol4 = TRANSPOSE_BLOCK_8( _blockA, 4 );    \\\n"
"const Dtype8    acol5 = TRANSPOSE_BLOCK_8( _blockA, 5 );    \\\n"
"const Dtype8    acol6 = TRANSPOSE_BLOCK_8( _blockA, 6 );    \\\n"
"const Dtype8    acol7 = TRANSPOSE_BLOCK_8( _blockA, 7 );    \\\n"
"_result = mad( (Dtype8)_blockB.s0, acol0, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s1, acol1, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s2, acol2, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s3, acol3, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s4, acol4, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s5, acol5, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s6, acol6, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s7, acol7, _result );      \\\n"
"}\n"
"#define GEMM_NT(ALPHA1, BETA_NOT0, VECSCALAR, VECSIZE) \\\n"
"__attribute__((intel_reqd_sub_group_size(SIMD_SIZE_GEMM))) \\\n"
"__attribute__((reqd_work_group_size(SIMD_SIZE_GEMM, 1, 1))) \\\n"
"__kernel void TEMPLATE(gemm_32_1_NT_ ##VECSCALAR ##_ ##ALPHA1 ##_ ##BETA_NOT0,Dtype)( \\\n"
"__read_only image2d_t A, \\\n"
"MATB_PARAMETER, \\\n"
"MATC_PARAMETER, \\\n"
"KERNEL_ARG_DTYPE alpha_in, \\\n"
"KERNEL_ARG_DTYPE beta_in, \\\n"
"int padded_k, \\\n"
"int k, \\\n"
"int isFirstColBlock) \\\n"
"{ \\\n"
"const Dtype alpha = (Dtype)alpha_in; \\\n"
"const Dtype beta = (Dtype)beta_in; \\\n"
"const int group_x = get_group_id(0); \\\n"
"const int group_y = get_group_id(1); \\\n"
"Dtype8 blockAxB00 = 0.0f; \\\n"
"Dtype8 blockAxB01 = 0.0f; \\\n"
"Dtype8 blockAxB02 = 0.0f; \\\n"
"Dtype8 blockAxB03 = 0.0f; \\\n"
"int2    coordA = (int2)( 0, group_y * TILE_M ); \\\n"
"int2    coordB = (int2)( 0, ( group_x * TILE_N )); \\\n"
"const sampler_t sampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST; \\\n"
"do \\\n"
"{ \\\n"
"Dtype8 blockB00;  \\\n"
"BLOCKB_READ8(blockB00, B, coordB); \\\n"
"int2    coordATemp = coordA; \\\n"
"Dtype8 blockA00 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8 blockA01 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8 blockA02 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.y += 8; \\\n"
"Dtype8 blockA03 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordA.x += TILE_K * SIZE_OF_ELEMENT; \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB00, blockA00, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB01, blockA01, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB02, blockA02, blockB00 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB03, blockA03, blockB00 ); \\\n"
"} \\\n"
"while( coordB.x < padded_k / VECSIZE ); \\\n"
"GEMM_OUTPUT(ALPHA1, BETA_NOT0); \\\n"
"}\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"_blockb.s0123 = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s4567 = READ_IMAGE(_B, _coordBTemp); _coordB.x += 2;\n"
"#define MATB_PARAMETER __read_only image2d_t B\n"
"GEMM_NT(1, 0, VEC4, 4)\n"
"GEMM_NT(1, 1, VEC4, 4)\n"
"GEMM_NT(0, 0, VEC4, 4)\n"
"GEMM_NT(0, 1, VEC4, 4)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"const __global Dtype *B_read = (__global Dtype *)(_B + (_coordBTemp.y * ldb) + _coordBTemp.x + offB); \\\n"
"_blockb = vload8(0, B_read); \\\n"
"_coordB.x += TILE_K;\n"
"#define MATB_PARAMETER __global Dtype *B, int offB, int ldb\n"
"GEMM_NT(1, 0, BUFFER, 1)\n"
"GEMM_NT(1, 1, BUFFER, 1)\n"
"GEMM_NT(0, 0, BUFFER, 1)\n"
"GEMM_NT(0, 1, BUFFER, 1)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"Dtype4 temp; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s0 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s1 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s2 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s3 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s4 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s5 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s6 = temp.s0; \\\n"
"temp = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s7 = temp.s0; \\\n"
"_coordB.x += 8;\n"
"#define MATB_PARAMETER __read_only image2d_t B\n"
"GEMM_NT(1, 0, SCALAR, 1)\n"
"GEMM_NT(1, 1, SCALAR, 1)\n"
"GEMM_NT(0, 0, SCALAR, 1)\n"
"GEMM_NT(0, 1, SCALAR, 1)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#undef MULTIPLY_BLOCKS_8x8\n"
"#undef TRANSPOSE_BLOCK_8\n"
"#undef GEMM_NT\n"
"#define TRANSPOSE_BLOCK_8(_vec, _col) \\\n"
"(Dtype8)( intel_sub_group_shuffle(_vec, _col + 0), \\\n"
"intel_sub_group_shuffle(_vec, _col + 1), \\\n"
"intel_sub_group_shuffle(_vec, _col + 2), \\\n"
"intel_sub_group_shuffle(_vec, _col + 3), \\\n"
"intel_sub_group_shuffle(_vec, _col + 4), \\\n"
"intel_sub_group_shuffle(_vec, _col + 5), \\\n"
"intel_sub_group_shuffle(_vec, _col + 6), \\\n"
"intel_sub_group_shuffle(_vec, _col + 7) );\n"
"#define MULTIPLY_BLOCKS_8x8( _result, _blockA, _blockB, _col )    \\\n"
"{   \\\n"
"const Dtype8    acol0 = TRANSPOSE_BLOCK_8( _blockA.s0, _col );    \\\n"
"const Dtype8    acol1 = TRANSPOSE_BLOCK_8( _blockA.s1, _col );    \\\n"
"const Dtype8    acol2 = TRANSPOSE_BLOCK_8( _blockA.s2, _col );    \\\n"
"const Dtype8    acol3 = TRANSPOSE_BLOCK_8( _blockA.s3, _col );    \\\n"
"const Dtype8    acol4 = TRANSPOSE_BLOCK_8( _blockA.s4, _col );    \\\n"
"const Dtype8    acol5 = TRANSPOSE_BLOCK_8( _blockA.s5, _col );    \\\n"
"const Dtype8    acol6 = TRANSPOSE_BLOCK_8( _blockA.s6, _col );    \\\n"
"const Dtype8    acol7 = TRANSPOSE_BLOCK_8( _blockA.s7, _col );    \\\n"
"_result = mad( (Dtype8)_blockB.s0, acol0, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s1, acol1, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s2, acol2, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s3, acol3, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s4, acol4, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s5, acol5, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s6, acol6, _result );      \\\n"
"_result = mad( (Dtype8)_blockB.s7, acol7, _result );      \\\n"
"}\n"
"#define GEMM_TT(ALPHA1, BETA_NOT0, VECSCALAR, VECSIZE) \\\n"
"__attribute__((intel_reqd_sub_group_size(SIMD_SIZE_GEMM))) \\\n"
"__attribute__((reqd_work_group_size(SIMD_SIZE_GEMM, 1, 1))) \\\n"
"__kernel void TEMPLATE(gemm_32_1_TT_ ##VECSCALAR ##_ ##ALPHA1 ##_ ##BETA_NOT0, Dtype)( \\\n"
"__read_only image2d_t A, \\\n"
"MATB_PARAMETER, \\\n"
"MATC_PARAMETER, \\\n"
"KERNEL_ARG_DTYPE alpha_in, \\\n"
"KERNEL_ARG_DTYPE beta_in, \\\n"
"int padded_k, \\\n"
"int k, \\\n"
"int isFirstColBlock) \\\n"
"{ \\\n"
"const Dtype alpha = (Dtype)alpha_in; \\\n"
"const Dtype beta = (Dtype)beta_in; \\\n"
"const int group_x = get_group_id(0); \\\n"
"const int group_y = get_group_id(1); \\\n"
"Dtype8 blockAxB00 = 0.0f; \\\n"
"Dtype8 blockAxB01 = 0.0f; \\\n"
"Dtype8 blockAxB02 = 0.0f; \\\n"
"Dtype8 blockAxB03 = 0.0f; \\\n"
"int2    coordA = (int2)( group_y * TILE_M * SIZE_OF_ELEMENT, 0 ); \\\n"
"int2    coordB = (int2)( 0, ( group_x * TILE_N )); \\\n"
"const sampler_t sampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST; \\\n"
"do \\\n"
"{ \\\n"
"Dtype8 blockB00;             \\\n"
"BLOCKB_READ8(blockB00, B, coordB); \\\n"
"int2    coordATemp = coordA; \\\n"
"Dtype8 blockA00 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT; \\\n"
"Dtype8 blockA01 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT; \\\n"
"Dtype8 blockA02 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordATemp.x += 8 * SIZE_OF_ELEMENT; \\\n"
"Dtype8 blockA03 = as_Dtype8( SUBGROUP_BLOCK_READ8( A, coordATemp ) );    coordA.y += TILE_K; \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB00, blockA00 , blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB01, blockA01 , blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB02, blockA02 , blockB00, 0 ); \\\n"
"MULTIPLY_BLOCKS_8x8( blockAxB03, blockA03 , blockB00, 0 ); \\\n"
"} \\\n"
"while( coordB.x < padded_k / VECSIZE ); \\\n"
"GEMM_OUTPUT(ALPHA1, BETA_NOT0);\\\n"
"}\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"_blockb.s0123 = READ_IMAGE(_B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s4567 = READ_IMAGE(_B, _coordBTemp); _coordB.x += 2;\n"
"#define MATB_PARAMETER __read_only image2d_t B\n"
"GEMM_TT(1, 0, VEC4, 4)\n"
"GEMM_TT(1, 1, VEC4, 4)\n"
"GEMM_TT(0, 0, VEC4, 4)\n"
"GEMM_TT(0, 1, VEC4, 4)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"const __global Dtype *B_read = (__global Dtype *)(_B + (_coordBTemp.y * k) + _coordBTemp.x + offB); \\\n"
"_blockb = vload8(0, B_read); \\\n"
"_coordB.x += TILE_K;\n"
"#define MATB_PARAMETER __global Dtype *B, int offB, int ldb\n"
"GEMM_TT(1, 0, BUFFER, 1)\n"
"GEMM_TT(1, 1, BUFFER, 1)\n"
"GEMM_TT(0, 0, BUFFER, 1)\n"
"GEMM_TT(0, 1, BUFFER, 1)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#define BLOCKB_READ8(_blockb, _B, _coordB) \\\n"
"int2 _coordBTemp = _coordB; \\\n"
"_coordBTemp.y += get_local_id(0); \\\n"
"Dtype4 temp; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s0 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s1 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s2 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s3 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s4 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s5 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s6 = temp.s0; \\\n"
"temp = READ_IMAGE(B, _coordBTemp); _coordBTemp.x += 1; \\\n"
"_blockb.s7 = temp.s0; \\\n"
"_coordB.x += 8;\n"
"#define MATB_PARAMETER __read_only image2d_t B\n"
"GEMM_TT(1, 0, SCALAR, 1)\n"
"GEMM_TT(1, 1, SCALAR, 1)\n"
"GEMM_TT(0, 0, SCALAR, 1)\n"
"GEMM_TT(0, 1, SCALAR, 1)\n"
"#undef BLOCKB_READ8\n"
"#undef MATB_PARAMETER\n"
"#undef MULTIPLY_BLOCKS_8x8\n"
"#undef TRANSPOSE_BLOCK_8\n"
"#undef GEMM_TT\n"
"#undef TILE_M\n"
"#undef TILE_K\n"
"#undef TILE_N\n"
"#undef SUBGROUP_BLOCK_READ8\n"
"#undef READ_IMAGE\n"
"#undef SIZE_OF_ELEMENT\n"
"__kernel void TEMPLATE(gemm_buffer_copy_image_transpose,Dtype)(\n"
"__global Dtype* A,\n"
"__write_only image2d_t ImA,\n"
"int offA,\n"
"int width,\n"
"int height,\n"
"int ldA)\n"
"{\n"
"const int gidx = get_global_id(0);\n"
"const int gidy = get_global_id(1);\n"
"int2 coord_dst = (int2)(gidx, gidy);\n"
"__global Dtype* A_off = A + offA;\n"
"Dtype srcA = A_off[gidy * ldA + gidx];\n"
"write_imagef(ImA, coord_dst, (Dtype4)srcA);\n"
"}\n"
"__kernel void TEMPLATE(gemm_buffer_copy_image_no_transpose,Dtype)(\n"
"__global Dtype* A,\n"
"__write_only image2d_t ImA,\n"
"int offA,\n"
"int width,\n"
"int height,\n"
"int ldA)\n"
"{\n"
"const int gidx = get_global_id(0);\n"
"const int gidy = get_global_id(1);\n"
"int2 coord_dst = (int2)(gidx, gidy);\n"
"if (gidx >= width || gidy >= height) {\n"
"write_imageui(ImA, coord_dst, (uint4)0);\n"
"return;\n"
"}\n"
"__global Dtype* A_off = A + offA;\n"
"uint4 srcA = convert_uint4(as_uchar4(A_off[gidy * ldA + gidx]));\n"
"write_imageui(ImA, coord_dst, srcA);\n"
"}\n"
, "f8c2215a3f6794f8e5dab7b9e63d2b38", NULL};
struct cv::ocl::internal::ProgramEntry im2col_oclsrc={moduleName, "im2col",
"__kernel void im2col(__global const T *im_src, int im_src_offset,\n"
"int channels, int height_inp, int width_inp,\n"
"int kernel_h, int kernel_w, int pad_h, int pad_w, int stride_h, int stride_w,\n"
"int height_out, int width_out,\n"
"__global T *im_col, int im_col_offset\n"
")\n"
"{\n"
"int index = get_global_id(0);\n"
"if (index >= height_out * width_out * channels)\n"
"return;\n"
"int j_out = index % width_out;\n"
"int i_out = (index / width_out) % height_out;\n"
"int c_inp = (index / width_out) / height_out;\n"
"int c_out = c_inp * kernel_h * kernel_w;\n"
"int i_inp = i_out * stride_h - pad_h;\n"
"int j_inp = j_out * stride_w - pad_w;\n"
"im_src += (c_inp * height_inp + i_inp) * width_inp + j_inp + im_src_offset;\n"
"im_col += (c_out * height_out + i_out) * width_out + j_out + im_col_offset;\n"
"for (int ki = 0; ki < kernel_h; ++ki)\n"
"for (int kj = 0; kj < kernel_w; ++kj) {\n"
"int i = i_inp + ki;\n"
"int j = j_inp + kj;\n"
"*im_col = (i >= 0 && j >= 0 && i < height_inp && j < width_inp) ?\n"
"im_src[ki * width_inp + kj] : 0;\n"
"im_col += height_out * width_out;\n"
"}\n"
"}\n"
, "609f199a321eef4535e1eff3ab281090", NULL};
struct cv::ocl::internal::ProgramEntry lrn_oclsrc={moduleName, "lrn",
"/*************************************************************************************\n"
"* Copyright (c) 2015, Advanced Micro Devices, Inc.\n"
"* All rights reserved.\n"
"*\n"
"* Redistribution and use in source and binary forms, with or without modification,\n"
"* are permitted provided that the following conditions are met:\n"
"*\n"
"* 1. Redistributions of source code must retain the above copyright notice, this\n"
"* list of conditions and the following disclaimer.\n"
"*\n"
"* 2. Redistributions in binary form must reproduce the above copyright notice,\n"
"* this list of conditions and the following disclaimer in the documentation and/or\n"
"*  other materials provided with the distribution.\n"
"*\n"
"* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n"
"* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n"
"* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n"
"* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n"
"* INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n"
"* BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n"
"* OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n"
"* WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n"
"* ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n"
"* POSSIBILITY OF SUCH DAMAGE.\n"
"**************************************************************************************/\n"
"__kernel void LRNComputeOutput(const int nthreads, __global T* in, __global T* scale, const T negative_beta, __global T* out) {\n"
"int index = get_global_id(0);\n"
"int tmp = get_global_size(0);\n"
"for(index; index < nthreads; index += tmp)\n"
"out[index] = in[index] * pow(scale[index], negative_beta);\n"
"}\n"
"__kernel void LRNFillScale(const int nthreads, __global T* in, const int num, const int channels, const int height, const int width, const int size, const T alpha_over_size, const T k, __global T* scale) {\n"
"int index = get_global_id(0);\n"
"int tmp = get_global_size(0);\n"
"for(index; index < nthreads; index += tmp) {\n"
"const int w = index % width;\n"
"const int h = (index / width) % height;\n"
"const int n = index / width / height;\n"
"const int offset = (n * channels * height + h) * width + w;\n"
"const int step = height * width;\n"
"in = in + offset;\n"
"scale = scale + offset;\n"
"int head = 0;\n"
"const int pre_pad = (size - 1) / 2;\n"
"const int post_pad = size - pre_pad - 1;\n"
"T accum_scale = 0;\n"
"while (head < post_pad && head < channels) {\n"
"accum_scale += in[head * step] * in[head * step];\n"
"++head;\n"
"}\n"
"while (head < channels) {\n"
"accum_scale += in[head * step] * in[head * step];\n"
"if (head - size >= 0) {\n"
"accum_scale -= in[(head - size) * step]\n"
"* in[(head - size) * step];\n"
"}\n"
"scale[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n"
"++head;\n"
"}\n"
"while (head < channels + post_pad) {\n"
"if (head - size >= 0) {\n"
"accum_scale -= in[(head - size) * step]\n"
"* in[(head - size) * step];\n"
"}\n"
"scale[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n"
"++head;\n"
"}\n"
"}\n"
"}\n"
, "0c65eb40713b6261f88bfa6731e32733", NULL};
struct cv::ocl::internal::ProgramEntry math_oclsrc={moduleName, "math",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"__kernel void TEMPLATE(axpy,Dtype)(const int n, const Dtype alpha, __global const Dtype* x,\n"
"const int offx, __global Dtype* y,\n"
"const int offy) {\n"
"for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n"
"Dtype src = x[offx + index];\n"
"Dtype dst = y[offy + index];\n"
"y[offy + index] = alpha * src + dst;\n"
"}\n"
"}\n"
, "4dafe535b2ca7e1001e516f6c46ca4af", NULL};
struct cv::ocl::internal::ProgramEntry matvec_mul_oclsrc={moduleName, "matvec_mul",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"__kernel void TEMPLATE(matvec_mul4,Dtype)(\n"
"__global const float * A,\n"
"int offA,\n"
"unsigned int A_col_size,\n"
"unsigned int trail_item,\n"
"__global const float * v,\n"
"int offv,\n"
"float alpha,\n"
"float beta,\n"
"__global float4 * result,\n"
"int offr,\n"
"__local float4 * work)\n"
"{\n"
"unsigned int row_gid = get_group_id(0);\n"
"unsigned int lid = get_local_id(0);\n"
"const __global float *src0_read = A + row_gid * 4 * A_col_size + offA;\n"
"const __global float *src1_read = v + offv;\n"
"result = (__global float4*)((__global float*)result + offr);\n"
"float4 dot0 = (float4)(0.f);\n"
"float4 dot1 = (float4)(0.f);\n"
"float4 dot2 = (float4)(0.f);\n"
"float4 dot3 = (float4)(0.f);\n"
"unsigned int i = lid;\n"
"while( i < A_col_size / 4) {\n"
"const float4 a0 = vload4(i, src0_read);\n"
"const float4 a1 = vload4(i, src0_read + A_col_size);\n"
"const float4 a2 = vload4(i, src0_read + 2 * A_col_size);\n"
"const float4 a3 = vload4(i, src0_read + 3 * A_col_size);\n"
"const float4 b0 = vload4(i, src1_read);\n"
"dot0 += a0 * b0;\n"
"dot1 += a1 * b0;\n"
"dot2 += a2 * b0;\n"
"dot3 += a3 * b0;\n"
"i += get_local_size(0);\n"
"}\n"
"work[lid].s0 = dot0.x + dot0.y + dot0.z + dot0.w;\n"
"work[lid].s1 = dot1.x + dot1.y + dot1.z + dot1.w;\n"
"work[lid].s2 = dot2.x + dot2.y + dot2.z + dot2.w;\n"
"work[lid].s3 = dot3.x + dot3.y + dot3.z + dot3.w;\n"
"if(i == A_col_size / 4)\n"
"{\n"
"if(trail_item != 0)\n"
"{\n"
"const __global float *src0_trail = src0_read + i * 4;\n"
"const __global float *src1_trail = src1_read + i * 4;\n"
"for(unsigned int i = 0; i < trail_item; ++i) {\n"
"const float at0 = src0_trail[i];\n"
"const float at1 = src0_trail[i + A_col_size];\n"
"const float at2 = src0_trail[i + 2 * A_col_size];\n"
"const float at3 = src0_trail[i + 3 * A_col_size];\n"
"const float bt = src1_trail[i];\n"
"work[lid].s0 += at0 * bt;\n"
"work[lid].s1 += at1 * bt;\n"
"work[lid].s2 += at2 * bt;\n"
"work[lid].s3 += at3 * bt;\n"
"}\n"
"}\n"
"}\n"
"for(unsigned int stride=get_local_size(0)/2 ; stride>0 ; stride>>=1) {\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"if(lid < stride)\n"
"work[lid] += work[lid+stride];\n"
"}\n"
"if(lid == 0) {\n"
"if(beta == (Dtype)0)\n"
"result[row_gid] = alpha * work[0];\n"
"else\n"
"result[row_gid] = alpha * work[0] + beta * result[row_gid];\n"
"}\n"
"}\n"
"__kernel void TEMPLATE(matvec_mul1,Dtype)(\n"
"__global const float * A,\n"
"int offA,\n"
"unsigned int A_col_size,\n"
"unsigned int row_offset,\n"
"unsigned int trail_item,\n"
"__global const float * v,\n"
"int offv,\n"
"float alpha,\n"
"float beta,\n"
"__global float * result,\n"
"int offr,\n"
"__local float * work)\n"
"{\n"
"unsigned int row_gid = get_group_id(0);\n"
"unsigned int lid = get_local_id(0);\n"
"const __global float *src0_read = A + (row_offset + row_gid) * A_col_size + offA;\n"
"const __global float *src1_read = v + + offv;\n"
"result = result + offr;\n"
"float4 dot0 = (float4)(0.f);\n"
"unsigned int i = lid;\n"
"while( i < A_col_size / 4)\n"
"{\n"
"const float4 a0 = vload4(i, src0_read);\n"
"const float4 b0 = vload4(i, src1_read);\n"
"dot0 += a0 * b0;\n"
"i += get_local_size(0);\n"
"}\n"
"work[lid] = dot0.x + dot0.y + dot0.z + dot0.w;\n"
"if(i == A_col_size / 4)\n"
"{\n"
"if(trail_item != 0)\n"
"{\n"
"const __global float *src0_trail = src0_read + i * 4;\n"
"const __global float *src1_trail = src1_read + i * 4;\n"
"for(unsigned int i = 0; i < trail_item; ++i) {\n"
"const float at0 = src0_trail[i];\n"
"const float bt = src1_trail[i];\n"
"work[lid] += at0 * bt;\n"
"}\n"
"}\n"
"}\n"
"for(unsigned int stride=get_local_size(0)/2 ; stride>0 ; stride>>=1) {\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"if(lid < stride)\n"
"work[lid] += work[lid+stride];\n"
"}\n"
"if(lid == 0) {\n"
"if(beta == (Dtype)0) {\n"
"result[row_gid+row_offset] = alpha * work[0];\n"
"} else {\n"
"result[row_gid+row_offset] *= beta;\n"
"result[row_gid+row_offset] += alpha * work[0];\n"
"}\n"
"}\n"
"}\n"
, "623e68f7c07dcab413ffa19f2b25c9eb", NULL};
struct cv::ocl::internal::ProgramEntry ocl4dnn_lrn_oclsrc={moduleName, "ocl4dnn_lrn",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"__kernel void TEMPLATE(lrn_full_no_scale,Dtype)(const int nthreads, __global const Dtype* in,\n"
"const int num, const int channels,\n"
"const int height, const int width, const int size,\n"
"const Dtype alpha_over_size, const Dtype k,\n"
"__global Dtype* const out,\n"
"const Dtype negative_beta) {\n"
"for (int index = get_global_id(0); index < nthreads;\n"
"index += get_global_size(0)) {\n"
"const int w = index % width;\n"
"const int h = (index / width) % height;\n"
"const int n = index / width / height;\n"
"const int offset = (n * channels * height + h) * width + w;\n"
"const int step = height * width;\n"
"__global const Dtype* in_off = in + offset;\n"
"__global Dtype* out_off = out + offset;\n"
"Dtype scale_val;\n"
"int head = 0;\n"
"const int pre_pad = (size - 1) / 2;\n"
"const int post_pad = size - pre_pad - 1;\n"
"Dtype accum_scale = 0;\n"
"while (head < post_pad && head < channels) {\n"
"accum_scale += in_off[head * step] * in_off[head * step];\n"
"++head;\n"
"}\n"
"while (head < channels) {\n"
"accum_scale += in_off[head * step] * in_off[head * step];\n"
"if (head - size >= 0) {\n"
"accum_scale -= in_off[(head - size) * step]\n"
"* in_off[(head - size) * step];\n"
"}\n"
"scale_val = k + accum_scale * alpha_over_size;\n"
"out_off[(head - post_pad) * step] = in_off[(head - post_pad) * step] * (Dtype)native_powr((float)scale_val, (float)negative_beta);\n"
"++head;\n"
"}\n"
"while (head < channels + post_pad) {\n"
"if (head - size >= 0) {\n"
"accum_scale -= in_off[(head - size) * step]\n"
"* in_off[(head - size) * step];\n"
"}\n"
"scale_val = k + accum_scale * alpha_over_size;\n"
"out_off[(head - post_pad) * step] = in_off[(head - post_pad) * step] * (Dtype)native_powr((float)scale_val, (float)negative_beta);\n"
"++head;\n"
"}\n"
"}\n"
"}\n"
, "df259e371515988c03201ecfc577be7c", NULL};
struct cv::ocl::internal::ProgramEntry ocl4dnn_pooling_oclsrc={moduleName, "ocl4dnn_pooling",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"void TEMPLATE(max_pool_forward_impl, Dtype)(\n"
"const int nthreads, __global const Dtype* bottom_data, const int num,\n"
"const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h,\n"
"const int kernel_w, const int stride_h, const int stride_w, const int pad_h,\n"
"const int pad_w,\n"
"__global Dtype* top_data,\n"
"const int use_mask, __global int* mask, __global Dtype* top_mask, bool no_mask)\n"
"{\n"
"for (int index = get_global_id(0); index < nthreads;\n"
"index += get_global_size(0))\n"
"{\n"
"const int pw = index % pooled_width;\n"
"const int ph = (index / pooled_width) % pooled_height;\n"
"const int c = (index / pooled_width / pooled_height) % channels;\n"
"const int n = index / pooled_width / pooled_height / channels;\n"
"int hstart = ph * stride_h - pad_h;\n"
"int wstart = pw * stride_w - pad_w;\n"
"const int hend = min(hstart + kernel_h, height);\n"
"const int wend = min(wstart + kernel_w, width);\n"
"hstart = max(hstart, (int)0);\n"
"wstart = max(wstart, (int)0);\n"
"Dtype maxval = -FLT_MAX;\n"
"int maxidx = -1;\n"
"__global const Dtype* bottom_slice = bottom_data\n"
"+ (n * channels + c) * height * width;\n"
"for (int h = hstart; h < hend; ++h) {\n"
"for (int w = wstart; w < wend; ++w) {\n"
"if (bottom_slice[h * width + w] > maxval) {\n"
"maxidx = h * width + w;\n"
"maxval = bottom_slice[maxidx];\n"
"}\n"
"}\n"
"}\n"
"top_data[index] = maxval;\n"
"if (!no_mask) {\n"
"if (use_mask == 1) {\n"
"mask[index] = maxidx;\n"
"} else {\n"
"top_mask[index] = maxidx;\n"
"}\n"
"}\n"
"}\n"
"}\n"
"__kernel void TEMPLATE(max_pool_forward, Dtype)(\n"
"const int nthreads, __global const Dtype* bottom_data, const int num,\n"
"const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h,\n"
"const int kernel_w, const int stride_h, const int stride_w, const int pad_h,\n"
"const int pad_w,\n"
"__global Dtype* top_data,\n"
"const int use_mask, __global int* mask, __global Dtype* top_mask)\n"
"{\n"
"TEMPLATE(max_pool_forward_impl, Dtype)(\n"
"nthreads, bottom_data, num, channels, height, width,\n"
"pooled_height, pooled_width, kernel_h,\n"
"kernel_w, stride_h, stride_w, pad_h, pad_w, top_data, use_mask, mask, top_mask, false\n"
");\n"
"}\n"
"__kernel void TEMPLATE(ave_pool_forward, Dtype)(\n"
"const int nthreads, __global const Dtype* const bottom_data, const int num,\n"
"const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h,\n"
"const int kernel_w, const int stride_h, const int stride_w, const int pad_h,\n"
"const int pad_w, __global Dtype* top_data)\n"
"{\n"
"for (int index = get_global_id(0); index < nthreads;\n"
"index += get_global_size(0))\n"
"{\n"
"{\n"
"const int pw = index % pooled_width;\n"
"const int ph = (index / pooled_width) % pooled_height;\n"
"const int c = (index / pooled_width / pooled_height) % channels;\n"
"const int n = index / pooled_width / pooled_height / channels;\n"
"int hstart = ph * stride_h - pad_h;\n"
"int wstart = pw * stride_w - pad_w;\n"
"int hend = min(hstart + kernel_h, height + pad_h);\n"
"int wend = min(wstart + kernel_w, width + pad_w);\n"
"const int pool_size = (hend - hstart) * (wend - wstart);\n"
"hstart = max(hstart, (int)0);\n"
"wstart = max(wstart, (int)0);\n"
"hend = min(hend, height);\n"
"wend = min(wend, width);\n"
"Dtype aveval = 0;\n"
"__global const Dtype* bottom_slice = bottom_data\n"
"+ (n * channels + c) * height * width;\n"
"for (int h = hstart; h < hend; ++h) {\n"
"for (int w = wstart; w < wend; ++w) {\n"
"aveval += bottom_slice[h * width + w];\n"
"}\n"
"}\n"
"top_data[index] = aveval / pool_size;\n"
"}\n"
"}\n"
"}\n"
"__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(\n"
"const int nthreads, __global const Dtype* const bottom_data, const int num,\n"
"const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h,\n"
"const int kernel_w, const int stride_h, const int stride_w,\n"
"__global Dtype* top_data)\n"
"{\n"
"for (int index = get_global_id(0); index < nthreads;\n"
"index += get_global_size(0))\n"
"{\n"
"const int pw = index % pooled_width;\n"
"const int ph = (index / pooled_width) % pooled_height;\n"
"const int c = (index / pooled_width / pooled_height) % channels;\n"
"const int n = index / pooled_width / pooled_height / channels;\n"
"const int hstart = ph * stride_h;\n"
"const int hend = min(hstart + kernel_h, height);\n"
"const int wstart = pw * stride_w;\n"
"const int wend = min(wstart + kernel_w, width);\n"
"Dtype cumsum = FLT_MIN;\n"
"Dtype cumvalues = 0.;\n"
"__global const Dtype* bottom_slice = bottom_data\n"
"+ (n * channels + c) * height * width;\n"
"for (int h = hstart; h < hend; ++h) {\n"
"for (int w = wstart; w < wend; ++w) {\n"
"cumsum += bottom_slice[h * width + w];\n"
"cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];\n"
"}\n"
"}\n"
"top_data[index] = cumvalues / cumsum;\n"
"}\n"
"}\n"
, "ed92c6999d11b80eb2d7b4e785ed454a", NULL};
struct cv::ocl::internal::ProgramEntry pooling_oclsrc={moduleName, "pooling",
"/*************************************************************************************\n"
"* Copyright (c) 2015, Advanced Micro Devices, Inc.\n"
"* All rights reserved.\n"
"*\n"
"* Redistribution and use in source and binary forms, with or without modification,\n"
"* are permitted provided that the following conditions are met:\n"
"*\n"
"* 1. Redistributions of source code must retain the above copyright notice, this\n"
"* list of conditions and the following disclaimer.\n"
"*\n"
"* 2. Redistributions in binary form must reproduce the above copyright notice,\n"
"* this list of conditions and the following disclaimer in the documentation and/or\n"
"*  other materials provided with the distribution.\n"
"*\n"
"* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n"
"* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n"
"* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n"
"* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n"
"* INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n"
"* BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n"
"* OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n"
"* WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n"
"* ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n"
"* POSSIBILITY OF SUCH DAMAGE.\n"
"**************************************************************************************/\n"
"__kernel void MaxPoolForward(const int nthreads,\n"
"__global T* bottom_data, const int num, const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h, const int kernel_w,\n"
"const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n"
"__global T* top_data\n"
"#ifdef MASK\n"
", __global float* mask\n"
"#endif\n"
")\n"
"{\n"
"int index = get_global_id(0);\n"
"int tmp = get_global_size(0);\n"
"for(index; index < nthreads; index += tmp) {\n"
"int pw = index % pooled_width;\n"
"int ph = (index / pooled_width) % pooled_height;\n"
"int c = (index / pooled_width / pooled_height) % channels;\n"
"int n = index / pooled_width / pooled_height / channels;\n"
"int hstart = ph * stride_h - pad_h;\n"
"int wstart = pw * stride_w - pad_w;\n"
"const int hend = min(hstart + kernel_h, height);\n"
"const int wend = min(wstart + kernel_w, width);\n"
"hstart = max(hstart, 0);\n"
"wstart = max(wstart, 0);\n"
"T maxval = -FLT_MAX;\n"
"int maxidx = -1;\n"
"bottom_data =\n"
"bottom_data + (n * channels + c) * height * width;\n"
"for (int h = hstart; h < hend; ++h) {\n"
"for (int w = wstart; w < wend; ++w) {\n"
"if (bottom_data[h * width + w] > maxval) {\n"
"maxidx = h * width + w;\n"
"maxval = bottom_data[maxidx];\n"
"}\n"
"}\n"
"}\n"
"top_data[index] = maxval;\n"
"#ifdef MASK\n"
"mask[index] = maxidx;\n"
"#endif\n"
"}\n"
"}\n"
"__kernel void AvePoolForward(const int nthreads,\n"
"__global T* bottom_data, const int num, const int channels, const int height, const int width,\n"
"const int pooled_height, const int pooled_width, const int kernel_h, const int kernel_w,\n"
"const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n"
"__global T* top_data\n"
"#ifdef MASK\n"
", __global float* mask\n"
"#endif\n"
")\n"
"{\n"
"int index = get_global_id(0);\n"
"int tmp = get_global_size(0);\n"
"for(index; index < nthreads; index+=tmp) {\n"
"int pw = index % pooled_width;\n"
"int ph = (index / pooled_width) % pooled_height;\n"
"int c = (index / pooled_width / pooled_height) % channels;\n"
"int n = index / pooled_width / pooled_height / channels; int hstart = ph * stride_h - pad_h; int wstart = pw * stride_w - pad_w;\n"
"int hend = min(hstart + kernel_h, height + pad_h);\n"
"int wend = min(wstart + kernel_w, width + pad_w);\n"
"const int pool_size = (hend - hstart) * (wend - wstart);\n"
"hstart = max(hstart, 0);\n"
"wstart = max(wstart, 0);\n"
"hend = min(hend, height);\n"
"wend = min(wend, width);\n"
"T aveval = 0;\n"
"bottom_data =\n"
"bottom_data + (n * channels + c) * height * width;\n"
"for (int h = hstart; h < hend; ++h) {\n"
"for (int w = wstart; w < wend; ++w) {\n"
"aveval += bottom_data[h * width + w];\n"
"}\n"
"}\n"
"top_data[index] = aveval / pool_size;\n"
"}\n"
"}\n"
, "c2329c23eb50e4f42ff1fd0a9aa1719c", NULL};
struct cv::ocl::internal::ProgramEntry softmax_oclsrc={moduleName, "softmax",
"/*************************************************************************************\n"
"* Copyright (c) 2015, Advanced Micro Devices, Inc.\n"
"* All rights reserved.\n"
"*\n"
"* Redistribution and use in source and binary forms, with or without modification,\n"
"* are permitted provided that the following conditions are met:\n"
"*\n"
"* 1. Redistributions of source code must retain the above copyright notice, this\n"
"* list of conditions and the following disclaimer.\n"
"*\n"
"* 2. Redistributions in binary form must reproduce the above copyright notice,\n"
"* this list of conditions and the following disclaimer in the documentation and/or\n"
"*  other materials provided with the distribution.\n"
"*\n"
"* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n"
"* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n"
"* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n"
"* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n"
"* INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n"
"* BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n"
"* OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n"
"* WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n"
"* ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n"
"* POSSIBILITY OF SUCH DAMAGE.\n"
"**************************************************************************************/\n"
"__kernel void kernel_channel_max(const int num, const int channels,\n"
"const int spatial_dim, __global const T* data, __global T* out) {\n"
"int index = get_global_id(0);\n"
"if(index < num * spatial_dim) {\n"
"int n = index / spatial_dim;\n"
"int s = index % spatial_dim;\n"
"T maxval = -FLT_MAX;\n"
"for (int c = 0; c < channels; ++c) {\n"
"maxval = max(data[(n * channels + c) * spatial_dim + s], maxval);\n"
"}\n"
"out[index] = maxval;\n"
"}\n"
"}\n"
"__kernel void kernel_channel_subtract(const int count,\n"
"const int num, const int channels,\n"
"const int spatial_dim, __global const T* channel_max, __global T* data) {\n"
"int index = get_global_id(0);\n"
"if(index < count) {\n"
"int n = index / channels / spatial_dim;\n"
"int s = index % spatial_dim;\n"
"data[index] -= channel_max[n * spatial_dim + s];\n"
"}\n"
"}\n"
"__kernel void kernel_channel_sum(const int num, const int channels,\n"
"const int spatial_dim, __global const T* data, __global T* channel_sum) {\n"
"int index = get_global_id(0);\n"
"if(index < num * spatial_dim) {\n"
"int n = index / spatial_dim;\n"
"int s = index % spatial_dim;\n"
"T sum = 0;\n"
"for (int c = 0; c < channels; ++c) {\n"
"sum += data[(n * channels + c) * spatial_dim + s];\n"
"}\n"
"channel_sum[index] = sum;\n"
"}\n"
"}\n"
"__kernel void kernel_channel_div(const int count,\n"
"const int num, const int channels,\n"
"const int spatial_dim, __global const T* channel_sum, __global T* data) {\n"
"int index = get_global_id(0);\n"
"if(index < count) {\n"
"int n = index / channels / spatial_dim;\n"
"int s = index % spatial_dim;\n"
"T v = data[index] / channel_sum[n * spatial_dim + s];\n"
"#ifdef LOG_SOFTMAX\n"
"v = log(v);\n"
"#endif\n"
"data[index] = v;\n"
"}\n"
"}\n"
, "31d417483a2f869aca3dcfcc3c2eff81", NULL};
struct cv::ocl::internal::ProgramEntry softmax_loss_oclsrc={moduleName, "softmax_loss",
"#define CONCAT(A,B) A##_##B\n"
"#define TEMPLATE(name,type) CONCAT(name,type)\n"
"#define Dtype float\n"
"#if defined(cl_intel_subgroups)\n"
"#pragma OPENCL EXTENSION  cl_intel_subgroups : enable\n"
"#endif\n"
"__kernel void TEMPLATE(softmax_forward_slm,Dtype)(const int num, const int channels,\n"
"const int spatial_dim,\n"
"__global Dtype* scale,\n"
"__global const Dtype* data,\n"
"__global Dtype* out,\n"
"__local Dtype *out_tmp,\n"
"__local Dtype *scale_tmp,\n"
"__local Dtype *group_tmp) {\n"
"int n = get_global_id(1);\n"
"for (int index = get_global_id(0), s = 0; index < spatial_dim * get_local_size(0); index +=\n"
"get_global_size(0), ++s) {\n"
"float maxval = -FLT_MAX;\n"
"for (int c = get_global_id(0); c < channels; c += get_global_size(0)) {\n"
"Dtype tmp = data[(n * channels + c) * spatial_dim + s];\n"
"maxval = max((Dtype)tmp, (Dtype)maxval);\n"
"}\n"
"maxval = sub_group_reduce_max(maxval * 100000);\n"
"group_tmp[get_sub_group_id() * spatial_dim + s] = maxval;\n"
"}\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < spatial_dim * get_max_sub_group_size(); index +=\n"
"get_global_size(0)) {\n"
"int s = index / get_max_sub_group_size();\n"
"Dtype maxval = sub_group_reduce_max(group_tmp[get_sub_group_local_id() * spatial_dim + s]);\n"
"scale_tmp[s] = maxval / 100000;\n"
"}\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < channels * spatial_dim;\n"
"index += get_global_size(0)) {\n"
"int s = index % spatial_dim;\n"
"out_tmp[index] = exp(data[n * channels * spatial_dim + index] - scale_tmp[s]);\n"
"}\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"for (int index = get_global_id(0), s = 0; index < spatial_dim * get_local_size(0); index +=\n"
"get_global_size(0), ++s) {\n"
"Dtype sum = 0;\n"
"for (int c = get_global_id(0); c < channels; c += get_global_size(0)) {\n"
"sum += out_tmp[c * spatial_dim + s];\n"
"}\n"
"sum = sub_group_reduce_add(sum * 100000);\n"
"group_tmp[get_sub_group_id() * spatial_dim + s] = sum;\n"
"}\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < spatial_dim * get_max_sub_group_size(); index +=\n"
"get_global_size(0)) {\n"
"int s = index / get_max_sub_group_size();\n"
"Dtype sum = sub_group_reduce_add(group_tmp[get_sub_group_local_id() * spatial_dim + s]);\n"
"scale_tmp[s] = sum / 100000;\n"
"}\n"
"barrier(CLK_LOCAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < channels * spatial_dim;\n"
"index += get_global_size(0)) {\n"
"int s = index % spatial_dim;\n"
"Dtype v = out_tmp[index] / scale_tmp[s];\n"
"#ifdef LOG_SOFTMAX\n"
"v = log(v);\n"
"#endif\n"
"out[n * channels * spatial_dim + index] = v;\n"
"}\n"
"}\n"
"__kernel void TEMPLATE(softmax_forward,Dtype)(const int num, const int channels,\n"
"const int spatial_dim,\n"
"__global Dtype* scale,\n"
"__global const Dtype* data,\n"
"__global Dtype* out) {\n"
"int n = get_global_id(1);\n"
"__global Dtype *group_tmp = scale + spatial_dim * num + n * get_max_sub_group_size() * spatial_dim;\n"
"for (int index = get_global_id(0), s = 0; index < spatial_dim * get_local_size(0); index +=\n"
"get_global_size(0), ++s) {\n"
"float maxval = -FLT_MAX;\n"
"for (int c = get_global_id(0); c < channels; c += get_global_size(0)) {\n"
"Dtype tmp = data[(n * channels + c) * spatial_dim + s];\n"
"maxval = max((Dtype)tmp, (Dtype)maxval);\n"
"}\n"
"maxval = sub_group_reduce_max(maxval * 100000);\n"
"group_tmp[get_sub_group_id() * spatial_dim + s] = maxval;\n"
"}\n"
"barrier(CLK_GLOBAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < spatial_dim * get_max_sub_group_size(); index +=\n"
"get_global_size(0)) {\n"
"int s = index / get_max_sub_group_size();\n"
"Dtype maxval = sub_group_reduce_max(group_tmp[get_sub_group_local_id() * spatial_dim + s]);\n"
"scale[n * spatial_dim + s] = maxval / 100000;\n"
"}\n"
"barrier(CLK_GLOBAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < channels * spatial_dim;\n"
"index += get_global_size(0)) {\n"
"int s = index % spatial_dim;\n"
"out[n * channels * spatial_dim + index] = exp(data[n * channels * spatial_dim + index] - scale[n * spatial_dim + s]);\n"
"}\n"
"barrier(CLK_GLOBAL_MEM_FENCE);\n"
"for (int index = get_global_id(0), s = 0; index < spatial_dim * get_local_size(0); index +=\n"
"get_global_size(0), ++s) {\n"
"Dtype sum = 0;\n"
"for (int c = get_global_id(0); c < channels; c += get_global_size(0)) {\n"
"sum += out[n * channels * spatial_dim + c * spatial_dim + s];\n"
"}\n"
"sum = sub_group_reduce_add(sum * 100000);\n"
"group_tmp[get_sub_group_id() * spatial_dim + s] = sum;\n"
"}\n"
"barrier(CLK_GLOBAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < spatial_dim * get_max_sub_group_size(); index +=\n"
"get_global_size(0)) {\n"
"int s = index / get_max_sub_group_size();\n"
"Dtype sum = sub_group_reduce_add(group_tmp[get_sub_group_local_id() * spatial_dim + s]);\n"
"scale[n * spatial_dim + s] = sum / 100000;\n"
"}\n"
"barrier(CLK_GLOBAL_MEM_FENCE);\n"
"for (int index = get_global_id(0); index < channels * spatial_dim;\n"
"index += get_global_size(0)) {\n"
"int s = index % spatial_dim;\n"
"Dtype v = out[n * channels * spatial_dim + index] / scale[n * spatial_dim + s];\n"
"#ifdef LOG_SOFTMAX\n"
"v = log(v);\n"
"#endif\n"
"out[n * channels * spatial_dim + index] = v;\n"
"}\n"
"}\n"
, "bed4a1d26ae3f68fcb04d3cc6cb6a878", NULL};

}}}
#endif
